{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T15:29:28.733243Z",
     "start_time": "2019-07-21T15:29:28.730839Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "## OpenAI Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### agent anatomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T15:49:38.486521Z",
     "start_time": "2019-07-21T15:49:38.475568Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward got: 3.5296\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.steps_left = 10\n",
    "\n",
    "    def get_observation(self):\n",
    "        return [0.0, 0.0, 0.0]\n",
    "\n",
    "    def get_actions(self):\n",
    "        return [0, 1]\n",
    "\n",
    "    def is_done(self):\n",
    "        return self.steps_left == 0\n",
    "\n",
    "    def action(self, action):\n",
    "        if self.is_done():\n",
    "            raise Exception(\"Game is over\")\n",
    "        self.steps_left -= 1\n",
    "        return random.random()\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "    def step(self, env):\n",
    "        current_obs = env.get_observation()\n",
    "        actions = env.get_actions()\n",
    "        reward = env.action(random.choice(actions))\n",
    "        self.total_reward += reward\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = Environment()\n",
    "    agent = Agent()\n",
    "\n",
    "    while not env.is_done():\n",
    "        agent.step(env)\n",
    "\n",
    "    print(\"Total reward got: %.4f\" % agent.total_reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### cartpole_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T15:58:25.857599Z",
     "start_time": "2019-07-21T15:58:25.850402Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode done in 42 steps, total reward 42.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drdh/anaconda3/envs/RL_hands_on_Env/lib/python3.7/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "    total_reward = 0.0\n",
    "    total_steps = 0\n",
    "    obs = env.reset() #reset the env and obtain the first observation\n",
    "\n",
    "    while True:\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        total_steps += 1\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print(\"Episode done in %d steps, total reward %.2f\" % (total_steps, total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### random_actionwrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T16:09:00.308519Z",
     "start_time": "2019-07-21T16:09:00.284598Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random!\n",
      "Random!\n",
      "Reward got: 10.00\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "\n",
    "class RandomActionWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env, epsilon=0.1):\n",
    "        super(RandomActionWrapper, self).__init__(env)\n",
    "        self.epsilon = epsilon #random ratio\n",
    "\n",
    "    def action(self, action): #action wrapper\n",
    "        if random.random() < self.epsilon:\n",
    "            print(\"Random!\")\n",
    "            return self.env.action_space.sample()\n",
    "        return action\n",
    "    \n",
    "    # def observation(self,obs):\n",
    "    # def reward(self,rew):\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = RandomActionWrapper(gym.make(\"CartPole-v0\"))\n",
    "\n",
    "    obs = env.reset()\n",
    "    total_reward = 0.0\n",
    "\n",
    "    while True:\n",
    "        obs, reward, done, _ = env.step(0) #same action 0\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print(\"Reward got: %.2f\" % total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### cartpole_random_monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If encounter error `attributeerror 'imagedata' object has no attribute 'data' gym`\n",
    "\n",
    "```bash\n",
    "pip install pyglet==1.3.2\n",
    "sudo apt-get install ffmpeg\n",
    "```\n",
    "\n",
    "will solve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T16:28:18.039519Z",
     "start_time": "2019-07-21T16:28:17.546548Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drdh/anaconda3/envs/RL_hands_on_Env/lib/python3.7/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode done in 12 steps, total reward 12.00\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    env = gym.wrappers.Monitor(env, \"recording\",force=True)# in ./recording/\n",
    "\n",
    "    total_reward = 0.0\n",
    "    total_steps = 0\n",
    "    obs = env.reset()\n",
    "\n",
    "    while True:\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        total_steps += 1\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print(\"Episode done in %d steps, total reward %.2f\" % (total_steps, total_reward))\n",
    "    env.close()\n",
    "    env.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Deep Learning with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[nn.module](https://pytorch.org/docs/stable/nn.html#module)\n",
    "\n",
    "All classes in the `torch.nn` packages inherit from the `nn.Module` base class.\n",
    "\n",
    "Some useful methods:\n",
    "\n",
    "- `parameters()`: A function that resturns iterator of all variables which require gradient computation(that is, module weights)\n",
    "- `zero_grad()`: initializes all gradients of all parameters to 0\n",
    "- `to(device)`: moves all module parameters to a given device(CPU/GPU)\n",
    "- `state_dict()`: returns the dictionary with all module parameters and is useful for model serialization\n",
    "- `load_state_dict()`: initializes the module with the state dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T05:00:50.496720Z",
     "start_time": "2019-07-22T05:00:50.487215Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OurModule(\n",
      "  (pipe): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=5, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=5, out_features=20, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=20, out_features=3, bias=True)\n",
      "    (5): Dropout(p=0.3)\n",
      "    (6): Softmax()\n",
      "  )\n",
      ")\n",
      "tensor([[0.4174, 0.4174, 0.1652],\n",
      "        [0.2642, 0.6622, 0.0735],\n",
      "        [0.2278, 0.7236, 0.0486]], grad_fn=<SoftmaxBackward>)\n",
      "Cuda's availability is False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class OurModule(nn.Module):\n",
    "    def __init__(self, num_inputs, num_classes, dropout_prob=0.3):\n",
    "        super(OurModule, self).__init__()\n",
    "        self.pipe = nn.Sequential(\n",
    "            nn.Linear(num_inputs, 5),\n",
    "            #torch.nn.Linear(in_features, out_features, bias=True)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, num_classes),\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Softmax(dim=1)\n",
    "            #along dim=1,since dim=0 is batch samples\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pipe(x)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    net = OurModule(num_inputs=2, num_classes=3)\n",
    "    print(net)\n",
    "    v = torch.FloatTensor([[2, 3],[3,4],[4,5]])#dim=0: batch, dim=1,input\n",
    "    out = net(v)\n",
    "    print(out)\n",
    "    print(\"Cuda's availability is %s\" % torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Data from cuda: %s\" % out.to('cuda'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T17:01:59.097811Z",
     "start_time": "2019-07-21T17:01:58.883837Z"
    },
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T08:19:42.813709Z",
     "start_time": "2019-07-22T08:19:42.689479Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    funcs = {\"sin\": math.sin, \"cos\": math.cos, \"tan\": math.tan}\n",
    "\n",
    "    for angle in range(-360, 360):\n",
    "        angle_rad = angle * math.pi / 180\n",
    "        for name, fun in funcs.items():\n",
    "            val = fun(angle_rad)\n",
    "            writer.add_scalar(name, val, angle)\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "then run \n",
    "```bash\n",
    "tensorboard --logdir runs --host localhost\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### atari_gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T08:51:09.281706Z",
     "start_time": "2019-07-22T08:37:40.085966Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Making new env: Breakout-v0\n",
      "INFO: Making new env: AirRaid-v0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drdh/anaconda3/envs/RL_hands_on_Env/lib/python3.7/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Making new env: Pong-v0\n",
      "INFO: Iter 100: gen_loss=4.538e+00, dis_loss=1.008e-01\n",
      "INFO: Iter 200: gen_loss=6.244e+00, dis_loss=6.144e-03\n",
      "INFO: Iter 300: gen_loss=6.966e+00, dis_loss=2.346e-03\n",
      "INFO: Iter 400: gen_loss=7.283e+00, dis_loss=2.378e-02\n",
      "INFO: Iter 500: gen_loss=6.235e+00, dis_loss=6.072e-02\n",
      "INFO: Iter 600: gen_loss=5.509e+00, dis_loss=2.121e-01\n",
      "INFO: Iter 700: gen_loss=5.555e+00, dis_loss=1.789e-02\n",
      "INFO: Iter 800: gen_loss=6.426e+00, dis_loss=6.950e-03\n",
      "INFO: Iter 900: gen_loss=6.289e+00, dis_loss=1.374e-01\n",
      "INFO: Iter 1000: gen_loss=5.316e+00, dis_loss=2.340e-02\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (30) : unknown error at /pytorch/aten/src/THC/THCGeneral.cpp:74",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-3fd63c29379f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mdis_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0miter_no\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mSAVE_IMAGE_EVERY_ITER\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fake\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_output_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"real\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RL_hands_on_Env/lib/python3.7/site-packages/tensorboardX/writer.py\u001b[0m in \u001b[0;36madd_image\u001b[0;34m(self, tag, img_tensor, global_step)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mimg_tensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mUse\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mto\u001b[0m \u001b[0mprepare\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mgood\u001b[0m \u001b[0midea\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \"\"\"\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msnd_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \"\"\"Add audio data to summary.\n",
      "\u001b[0;32m~/anaconda3/envs/RL_hands_on_Env/lib/python3.7/site-packages/tensorboardX/summary.py\u001b[0m in \u001b[0;36mimage\u001b[0;34m(tag, tensor)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_clean_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'input tensor should be one of numpy.ndarray, torch.cuda.FloatTensor, torch.FloatTensor'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m4\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'input tensor should be 3 dimensional.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (30) : unknown error at /pytorch/aten/src/THC/THCGeneral.cpp:74"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import random\n",
    "import argparse\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import gym\n",
    "import gym.spaces\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "log = gym.logger\n",
    "log.set_level(gym.logger.INFO)\n",
    "\n",
    "LATENT_VECTOR_SIZE = 100\n",
    "DISCR_FILTERS = 64\n",
    "GENER_FILTERS = 64\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# dimension input image will be rescaled\n",
    "IMAGE_SIZE = 64\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "REPORT_EVERY_ITER = 100\n",
    "SAVE_IMAGE_EVERY_ITER = 1000\n",
    "\n",
    "\n",
    "class InputWrapper(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Preprocessing of input numpy array:\n",
    "    1. resize image into predefined size\n",
    "    2. move color channel axis to a first place\n",
    "    \"\"\"\n",
    "    def __init__(self, *args):\n",
    "        super(InputWrapper, self).__init__(*args)\n",
    "        assert isinstance(self.observation_space, gym.spaces.Box)\n",
    "        old_space = self.observation_space\n",
    "        self.observation_space = gym.spaces.Box(self.observation(old_space.low), self.observation(old_space.high),\n",
    "                                                dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        #1. resize image #(210,160) -> (64,64)\n",
    "        new_obs = cv2.resize(observation, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "        #2. transform (210, 160, 3) -> (3, 210, 160) i.e.(channels,height,width)\n",
    "        new_obs = np.moveaxis(new_obs, 2, 0)\n",
    "        return new_obs.astype(np.float32)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # this pipe converges image into the single number\n",
    "        self.conv_pipe = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape[0], out_channels=DISCR_FILTERS,\n",
    "                      kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=DISCR_FILTERS, out_channels=DISCR_FILTERS*2,\n",
    "                      kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(DISCR_FILTERS*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=DISCR_FILTERS * 2, out_channels=DISCR_FILTERS * 4,\n",
    "                      kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(DISCR_FILTERS * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=DISCR_FILTERS * 4, out_channels=DISCR_FILTERS * 8,\n",
    "                      kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(DISCR_FILTERS * 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=DISCR_FILTERS * 8, out_channels=1,\n",
    "                      kernel_size=4, stride=1, padding=0),\n",
    "            #real/fake probability\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv_pipe(x)\n",
    "        return conv_out.view(-1, 1).squeeze(dim=1)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, output_shape):\n",
    "        super(Generator, self).__init__()\n",
    "        # pipe deconvolves input vector into (3, 64, 64) image\n",
    "        self.pipe = nn.Sequential(\n",
    "            #input: lantent vector\n",
    "            #transposed convolution: deconvolution\n",
    "            nn.ConvTranspose2d(in_channels=LATENT_VECTOR_SIZE, out_channels=GENER_FILTERS * 8,\n",
    "                               kernel_size=4, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(GENER_FILTERS * 8),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=GENER_FILTERS * 8, out_channels=GENER_FILTERS * 4,\n",
    "                               kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(GENER_FILTERS * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=GENER_FILTERS * 4, out_channels=GENER_FILTERS * 2,\n",
    "                               kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(GENER_FILTERS * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=GENER_FILTERS * 2, out_channels=GENER_FILTERS,\n",
    "                               kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(GENER_FILTERS),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=GENER_FILTERS, out_channels=output_shape[0],\n",
    "                               kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pipe(x)\n",
    "\n",
    "\n",
    "    #infinitely samples\n",
    "def iterate_batches(envs, batch_size=BATCH_SIZE):\n",
    "    batch = [e.reset() for e in envs]\n",
    "    env_gen = iter(lambda: random.choice(envs), None)\n",
    "\n",
    "    while True: \n",
    "        e = next(env_gen)\n",
    "        obs, reward, is_done, _ = e.step(e.action_space.sample()) #play by random agent\n",
    "        if np.mean(obs) > 0.01:\n",
    "            batch.append(obs)\n",
    "        if len(batch) == batch_size:\n",
    "            # Normalising input between -1 to 1\n",
    "            batch_np = np.array(batch, dtype=np.float32) * 2.0 / 255.0 - 1.0\n",
    "            yield torch.tensor(batch_np,dtype=torch.float32)\n",
    "            batch.clear()\n",
    "        if is_done:\n",
    "            e.reset()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #parser = argparse.ArgumentParser()\n",
    "    #parser.add_argument(\"--cuda\", default=False, action='store_true', help=\"Enable cuda computation\")\n",
    "    #args = parser.parse_args()\n",
    "\n",
    "    #device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "    device=torch.device(\"cpu\")\n",
    "    envs = [InputWrapper(gym.make(name)) for name in ('Breakout-v0', 'AirRaid-v0', 'Pong-v0')]\n",
    "    input_shape = envs[0].observation_space.shape\n",
    "    \n",
    "    #2 nets\n",
    "    net_discr = Discriminator(input_shape=input_shape).to(device)\n",
    "    net_gener = Generator(output_shape=input_shape).to(device)\n",
    "\n",
    "    objective = nn.BCELoss()\n",
    "    gen_optimizer = optim.Adam(params=net_gener.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "    dis_optimizer = optim.Adam(params=net_discr.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    gen_losses = []\n",
    "    dis_losses = []\n",
    "    iter_no = 0\n",
    "\n",
    "    true_labels_v = torch.ones(BATCH_SIZE, dtype=torch.float32, device=device)\n",
    "    fake_labels_v = torch.zeros(BATCH_SIZE, dtype=torch.float32, device=device)\n",
    "\n",
    "    for batch_v in iterate_batches(envs):\n",
    "        # generate extra fake samples, input is 4D: batch, filters, x, y\n",
    "        gen_input_v = torch.FloatTensor(BATCH_SIZE, LATENT_VECTOR_SIZE, 1, 1).normal_(0, 1).to(device)\n",
    "        batch_v = batch_v.to(device)\n",
    "        gen_output_v = net_gener(gen_input_v)\n",
    "\n",
    "        # train discriminator\n",
    "        dis_optimizer.zero_grad()\n",
    "        dis_output_true_v = net_discr(batch_v) #true data samples\n",
    "        dis_output_fake_v = net_discr(gen_output_v.detach()) #generated samples,detach():a copy(), no gradient flow\n",
    "        dis_loss = objective(dis_output_true_v, true_labels_v) + objective(dis_output_fake_v, fake_labels_v)\n",
    "        dis_loss.backward()\n",
    "        dis_optimizer.step()\n",
    "        dis_losses.append(dis_loss.item())\n",
    "\n",
    "        # train generator\n",
    "        gen_optimizer.zero_grad()\n",
    "        dis_output_v = net_discr(gen_output_v)\n",
    "        gen_loss_v = objective(dis_output_v, true_labels_v)\n",
    "        gen_loss_v.backward()\n",
    "        gen_optimizer.step()\n",
    "        gen_losses.append(gen_loss_v.item())\n",
    "\n",
    "        #for tensorboard: tensorboard --logdir runs --host localhost\n",
    "        iter_no += 1\n",
    "        if iter_no % REPORT_EVERY_ITER == 0:\n",
    "            log.info(\"Iter %d: gen_loss=%.3e, dis_loss=%.3e\", iter_no, np.mean(gen_losses), np.mean(dis_losses))\n",
    "            writer.add_scalar(\"gen_loss\", np.mean(gen_losses), iter_no)\n",
    "            writer.add_scalar(\"dis_loss\", np.mean(dis_losses), iter_no)\n",
    "            gen_losses = []\n",
    "            dis_losses = []\n",
    "        if iter_no % SAVE_IMAGE_EVERY_ITER == 0:\n",
    "            writer.add_image(\"fake\", vutils.make_grid(gen_output_v.data[:64], normalize=True), iter_no)\n",
    "            writer.add_image(\"real\", vutils.make_grid(batch_v.data[:64], normalize=True), iter_no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T02:37:28.468404Z",
     "start_time": "2019-07-22T02:37:28.457250Z"
    },
    "hidden": true
   },
   "source": [
    "```bash\n",
    "tensorboard --logdir runs --host localhost\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## The Cross-Entropy Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. sample $A_1,\\cdots,A_N$ from $p(A)$\n",
    "2. evaluate $J(A_1),\\cdots,J(A_N)$\n",
    "3. pick the **elites** $A_{i_1},\\cdots,A_{i_M}$ with the highest value, where $M<N$\n",
    "4. refit $p(A)$ to the elites $A_{i_1},\\cdots,A_{i_M}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T15:54:58.070144Z",
     "start_time": "2019-07-22T15:54:39.971962Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=0.694, reward_mean=20.2, reward_bound=22.0\n",
      "1: loss=0.694, reward_mean=28.0, reward_bound=32.0\n",
      "2: loss=0.687, reward_mean=24.2, reward_bound=29.0\n",
      "3: loss=0.670, reward_mean=23.6, reward_bound=23.0\n",
      "4: loss=0.661, reward_mean=27.9, reward_bound=32.5\n",
      "5: loss=0.661, reward_mean=37.3, reward_bound=43.0\n",
      "6: loss=0.658, reward_mean=43.8, reward_bound=55.0\n",
      "7: loss=0.650, reward_mean=45.3, reward_bound=58.0\n",
      "8: loss=0.629, reward_mean=30.8, reward_bound=33.5\n",
      "9: loss=0.638, reward_mean=35.1, reward_bound=39.5\n",
      "10: loss=0.629, reward_mean=45.6, reward_bound=54.0\n",
      "11: loss=0.607, reward_mean=49.8, reward_bound=52.5\n",
      "12: loss=0.623, reward_mean=42.6, reward_bound=46.0\n",
      "13: loss=0.608, reward_mean=52.8, reward_bound=60.5\n",
      "14: loss=0.621, reward_mean=49.2, reward_bound=60.5\n",
      "15: loss=0.595, reward_mean=47.5, reward_bound=54.0\n",
      "16: loss=0.591, reward_mean=48.5, reward_bound=55.5\n",
      "17: loss=0.593, reward_mean=52.3, reward_bound=57.5\n",
      "18: loss=0.580, reward_mean=62.2, reward_bound=67.5\n",
      "19: loss=0.577, reward_mean=74.2, reward_bound=88.0\n",
      "20: loss=0.583, reward_mean=62.1, reward_bound=73.0\n",
      "21: loss=0.571, reward_mean=58.2, reward_bound=63.0\n",
      "22: loss=0.570, reward_mean=69.9, reward_bound=81.5\n",
      "23: loss=0.590, reward_mean=72.9, reward_bound=85.0\n",
      "24: loss=0.559, reward_mean=76.8, reward_bound=88.5\n",
      "25: loss=0.548, reward_mean=72.1, reward_bound=80.0\n",
      "26: loss=0.553, reward_mean=88.8, reward_bound=99.5\n",
      "27: loss=0.548, reward_mean=91.6, reward_bound=102.5\n",
      "28: loss=0.547, reward_mean=77.9, reward_bound=81.5\n",
      "29: loss=0.549, reward_mean=98.6, reward_bound=108.0\n",
      "30: loss=0.542, reward_mean=112.9, reward_bound=135.5\n",
      "31: loss=0.523, reward_mean=103.1, reward_bound=123.0\n",
      "32: loss=0.548, reward_mean=105.4, reward_bound=136.0\n",
      "33: loss=0.525, reward_mean=123.1, reward_bound=145.0\n",
      "34: loss=0.535, reward_mean=159.7, reward_bound=183.5\n",
      "35: loss=0.530, reward_mean=143.9, reward_bound=153.0\n",
      "36: loss=0.544, reward_mean=141.3, reward_bound=167.5\n",
      "37: loss=0.522, reward_mean=169.8, reward_bound=200.0\n",
      "38: loss=0.519, reward_mean=183.4, reward_bound=200.0\n",
      "39: loss=0.529, reward_mean=190.8, reward_bound=200.0\n",
      "40: loss=0.522, reward_mean=192.9, reward_bound=200.0\n",
      "41: loss=0.530, reward_mean=193.4, reward_bound=200.0\n",
      "42: loss=0.524, reward_mean=194.9, reward_bound=200.0\n",
      "43: loss=0.527, reward_mean=200.0, reward_bound=200.0\n",
      "Solved!\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "HIDDEN_SIZE = 128 #random\n",
    "BATCH_SIZE = 16\n",
    "PERCENTILE = 70 #only leave top 30% \n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)# probability distribution over action\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "#helper\n",
    "Episode = namedtuple('Episode', field_names=['reward', 'steps']) #for a single episode\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])#for a single step\n",
    "\n",
    "\n",
    "def iterate_batches(env, net, batch_size):\n",
    "    batch = [] # to accumulate batch\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    obs = env.reset()\n",
    "    sm = nn.Softmax(dim=1) #softmax layer\n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor([obs])# (4) -> (1,4), add batch axis\n",
    "        act_probs_v = sm(net(obs_v)) #raw action scores are fed to softmax function\n",
    "        act_probs = act_probs_v.data.numpy()[0] #unpack tensors from gradient track\n",
    "        action = np.random.choice(len(act_probs), p=act_probs) #int a=np.arange(len(...)),p=probability\n",
    "        next_obs, reward, is_done, _ = env.step(action)\n",
    "        episode_reward += reward #total reward of this episode\n",
    "        episode_steps.append(EpisodeStep(observation=obs, action=action))\n",
    "        if is_done:\n",
    "            batch.append(Episode(reward=episode_reward, steps=episode_steps))#an episode is over\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            next_obs = env.reset()\n",
    "            if len(batch) == batch_size:#number of episode == batch_size\n",
    "                yield batch\n",
    "                batch = []\n",
    "        obs = next_obs #current obs\n",
    "\n",
    "\n",
    "def filter_batch(batch, percentile):\n",
    "    rewards = list(map(lambda s: s.reward,  batch))# get every episode's reward\n",
    "    reward_bound = np.percentile(rewards, percentile) #boundary reward, to filter elite episode\n",
    "    reward_mean = float(np.mean(rewards))\n",
    "\n",
    "    train_obs = [] # long long observations\n",
    "    train_act = [] # long long actions\n",
    "    for example in batch:\n",
    "        if example.reward < reward_bound:\n",
    "            continue # filter\n",
    "        train_obs.extend(map(lambda step: step.observation, example.steps))\n",
    "        train_act.extend(map(lambda step: step.action, example.steps))\n",
    "\n",
    "    train_obs_v = torch.FloatTensor(train_obs)\n",
    "    train_act_v = torch.LongTensor(train_act)\n",
    "    return train_obs_v, train_act_v, reward_bound, reward_mean # the last two are used in TensorBoard\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    env = gym.wrappers.Monitor(env, directory=\"mon\", force=True) #video\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "    objective = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(params=net.parameters(), lr=0.01)\n",
    "    writer = SummaryWriter(comment=\"-cartpole\")\n",
    "\n",
    "    for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
    "        obs_v, acts_v, reward_b, reward_m = filter_batch(batch, PERCENTILE)\n",
    "        optimizer.zero_grad()\n",
    "        action_scores_v = net(obs_v)\n",
    "        loss_v = objective(action_scores_v, acts_v) # fit Net over action distribution\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "        print(\"%d: loss=%.3f, reward_mean=%.1f, reward_bound=%.1f\" % (\n",
    "            iter_no, loss_v.item(), reward_m, reward_b))\n",
    "        writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
    "        writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n",
    "        writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n",
    "        if reward_m > 199:\n",
    "            print(\"Solved!\")\n",
    "            break\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### frozenlake naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T16:27:14.245802Z",
     "start_time": "2019-07-22T16:27:14.240121Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Discrete(16), Discrete(4), 0, None)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "e=gym.make(\"FrozenLake-v0\")\n",
    "e.observation_space,e.action_space,e.reset(),e.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T16:50:44.577203Z",
     "start_time": "2019-07-22T16:50:43.820318Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=1.399, reward_mean=0.0, reward_bound=0.0\n",
      "1: loss=1.373, reward_mean=0.0, reward_bound=0.0\n",
      "2: loss=1.350, reward_mean=0.2, reward_bound=0.0\n",
      "3: loss=1.337, reward_mean=0.0, reward_bound=0.0\n",
      "4: loss=1.332, reward_mean=0.0, reward_bound=0.0\n",
      "5: loss=1.333, reward_mean=0.0, reward_bound=0.0\n",
      "6: loss=1.286, reward_mean=0.0, reward_bound=0.0\n",
      "7: loss=1.290, reward_mean=0.0, reward_bound=0.0\n",
      "8: loss=1.295, reward_mean=0.0, reward_bound=0.0\n",
      "9: loss=1.261, reward_mean=0.0, reward_bound=0.0\n",
      "10: loss=1.321, reward_mean=0.0, reward_bound=0.0\n",
      "11: loss=1.245, reward_mean=0.0, reward_bound=0.0\n",
      "12: loss=1.323, reward_mean=0.0, reward_bound=0.0\n",
      "13: loss=1.265, reward_mean=0.0, reward_bound=0.0\n",
      "14: loss=1.282, reward_mean=0.0, reward_bound=0.0\n",
      "15: loss=1.229, reward_mean=0.0, reward_bound=0.0\n",
      "16: loss=1.129, reward_mean=0.0, reward_bound=0.0\n",
      "17: loss=1.256, reward_mean=0.0, reward_bound=0.0\n",
      "18: loss=1.163, reward_mean=0.0, reward_bound=0.0\n",
      "19: loss=1.181, reward_mean=0.0, reward_bound=0.0\n",
      "20: loss=1.170, reward_mean=0.0, reward_bound=0.0\n",
      "21: loss=1.048, reward_mean=0.0, reward_bound=0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b538fac60e60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"-frozenlake-naive\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0miter_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterate_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0mobs_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macts_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPERCENTILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-b538fac60e60>\u001b[0m in \u001b[0;36miterate_batches\u001b[0;34m(env, net, batch_size)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mact_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mact_probs_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mact_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mnext_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mepisode_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mepisode_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEpisodeStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36m_after_step\u001b[0;34m(self, observation, reward, done, info)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;31m# Record stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0;31m# Record video\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/wrappers/monitoring/stats_recorder.py\u001b[0m in \u001b[0;36mafter_step\u001b[0;34m(self, observation, reward, done, info)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym, gym.spaces\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "PERCENTILE = 70\n",
    "\n",
    "\n",
    "class DiscreteOneHotWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(DiscreteOneHotWrapper, self).__init__(env)\n",
    "        assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
    "        self.observation_space = gym.spaces.Box(0.0, 1.0, (env.observation_space.n, ), dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        res = np.copy(self.observation_space.low)\n",
    "        res[observation] = 1.0\n",
    "        return res\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])\n",
    "\n",
    "\n",
    "def iterate_batches(env, net, batch_size):\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    obs = env.reset()\n",
    "    sm = nn.Softmax(dim=1)\n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor([obs])\n",
    "        act_probs_v = sm(net(obs_v))\n",
    "        act_probs = act_probs_v.data.numpy()[0]\n",
    "        action = np.random.choice(len(act_probs), p=act_probs)\n",
    "        next_obs, reward, is_done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        episode_steps.append(EpisodeStep(observation=obs, action=action))\n",
    "        if is_done:\n",
    "            batch.append(Episode(reward=episode_reward, steps=episode_steps))\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            next_obs = env.reset()\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        obs = next_obs\n",
    "\n",
    "\n",
    "def filter_batch(batch, percentile):\n",
    "    rewards = list(map(lambda s: s.reward, batch))\n",
    "    reward_bound = np.percentile(rewards, percentile)\n",
    "    reward_mean = float(np.mean(rewards))\n",
    "\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    for example in batch:\n",
    "        if example.reward < reward_bound:\n",
    "            continue\n",
    "        train_obs.extend(map(lambda step: step.observation, example.steps))\n",
    "        train_act.extend(map(lambda step: step.action, example.steps))\n",
    "\n",
    "    train_obs_v = torch.FloatTensor(train_obs)\n",
    "    train_act_v = torch.LongTensor(train_act)\n",
    "    return train_obs_v, train_act_v, reward_bound, reward_mean\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = DiscreteOneHotWrapper(gym.make(\"FrozenLake-v0\"))\n",
    "    env = gym.wrappers.Monitor(env, directory=\"mon\", force=True)\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "    objective = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(params=net.parameters(), lr=0.01)\n",
    "    writer = SummaryWriter(comment=\"-frozenlake-naive\")\n",
    "\n",
    "    for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
    "        obs_v, acts_v, reward_b, reward_m = filter_batch(batch, PERCENTILE)\n",
    "        optimizer.zero_grad()\n",
    "        action_scores_v = net(obs_v)\n",
    "        loss_v = objective(action_scores_v, acts_v)\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "        print(\"%d: loss=%.3f, reward_mean=%.1f, reward_bound=%.1f\" % (\n",
    "            iter_no, loss_v.item(), reward_m, reward_b))\n",
    "        writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
    "        writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n",
    "        writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n",
    "        if reward_m > 0.8:\n",
    "            print(\"Solved!\")\n",
    "            break\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In CartPole, every step of the environment gives us the\n",
    "reward 1.0, until the moment that the pole falls. So, the longer our agent\n",
    "balanced the pole, the more reward it obtained. Due to randomness in our agent's\n",
    "behavior, different episodes were of different lengths, which gave us a pretty\n",
    "normal distribution of the episodes' rewards. After choosing a reward boundary,\n",
    "we rejected less successful episodes and learned how to repeat better ones (by\n",
    "training on successful episodes' data).\n",
    "\n",
    "In the FrozenLake environment, episodes and their reward look different. We get\n",
    "the reward of 1.0 only when we reach the goal, and this reward says nothing\n",
    "about how good each episode was. Was it quick and efficient or did we make\n",
    "four rounds on the lake before we randomly stepped into the final cell? We don't\n",
    "know, it's just 1.0 reward and that's it. The distribution of rewards for our\n",
    "episodes are also problematic. There are only two kinds of episodes possible,\n",
    "with zero reward (failed) and one reward (successful), and failed episodes will\n",
    "obviously dominate in the beginning of the training. So, our percentile selection\n",
    "of \"elite\" episodes is totally wrong and gives us bad examples to train on. This is\n",
    "the reason for our training failure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### frozenlake tweaked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- **Larger batches of played episodes**: FrozenLake requires at least 100 just to get some successful episodes.\n",
    "- **Discount factor applied to reward**: To make the total reward for the episode depend on episode length, and add variety in episodes, In this case, the reward for shorter episodes will be higher than the reward for longer ones\n",
    "- **Keeping \"elite\" episodes for a longer time**:In FrozenLake, a successful episode is a much rarer animal, so we need to keep them for several iterations to train on them.\n",
    "- **Decrease learning rate**: This will give our network time to average more training samples.\n",
    "- **Much longer training time**: Due to the sparsity of successful episodes, and the random outcome of our actions, it's much harder for our network to get an idea of the best behavior to perform in any particular situation. To reach 50% successful episodes, about 5k training iterations are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T17:55:34.722829Z",
     "start_time": "2019-07-22T17:07:57.959847Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=1.371, reward_mean=0.010, reward_bound=0.000, batch=1\n",
      "100: loss=1.280, reward_mean=0.040, reward_bound=0.000, batch=181\n",
      "200: loss=1.165, reward_mean=0.070, reward_bound=0.254, batch=225\n",
      "300: loss=1.075, reward_mean=0.040, reward_bound=0.277, batch=229\n",
      "400: loss=1.071, reward_mean=0.060, reward_bound=0.000, batch=139\n",
      "500: loss=1.024, reward_mean=0.090, reward_bound=0.183, batch=227\n",
      "600: loss=1.011, reward_mean=0.060, reward_bound=0.045, batch=221\n",
      "700: loss=1.033, reward_mean=0.140, reward_bound=0.314, batch=220\n",
      "800: loss=1.021, reward_mean=0.030, reward_bound=0.229, batch=229\n",
      "900: loss=0.783, reward_mean=0.170, reward_bound=0.185, batch=217\n",
      "1000: loss=0.753, reward_mean=0.130, reward_bound=0.349, batch=226\n",
      "1100: loss=0.763, reward_mean=0.110, reward_bound=0.080, batch=219\n",
      "1200: loss=0.728, reward_mean=0.200, reward_bound=0.351, batch=228\n",
      "1300: loss=0.719, reward_mean=0.160, reward_bound=0.254, batch=226\n",
      "1400: loss=0.722, reward_mean=0.110, reward_bound=0.405, batch=230\n",
      "1500: loss=0.600, reward_mean=0.130, reward_bound=0.077, batch=214\n",
      "1600: loss=0.568, reward_mean=0.160, reward_bound=0.109, batch=220\n",
      "1700: loss=0.533, reward_mean=0.250, reward_bound=0.150, batch=212\n",
      "1800: loss=0.582, reward_mean=0.120, reward_bound=0.249, batch=229\n",
      "1900: loss=0.547, reward_mean=0.260, reward_bound=0.274, batch=224\n",
      "2000: loss=0.545, reward_mean=0.140, reward_bound=0.325, batch=229\n",
      "2100: loss=0.462, reward_mean=0.230, reward_bound=0.089, batch=215\n",
      "2200: loss=0.418, reward_mean=0.360, reward_bound=0.324, batch=225\n",
      "2300: loss=0.416, reward_mean=0.340, reward_bound=0.324, batch=225\n",
      "2400: loss=0.414, reward_mean=0.480, reward_bound=0.387, batch=184\n",
      "2500: loss=0.388, reward_mean=0.260, reward_bound=0.347, batch=231\n",
      "2600: loss=0.365, reward_mean=0.340, reward_bound=0.308, batch=227\n",
      "2700: loss=0.370, reward_mean=0.350, reward_bound=0.439, batch=231\n",
      "2800: loss=0.333, reward_mean=0.390, reward_bound=0.229, batch=216\n",
      "2900: loss=0.311, reward_mean=0.420, reward_bound=0.387, batch=225\n",
      "3000: loss=0.318, reward_mean=0.440, reward_bound=0.265, batch=223\n",
      "3100: loss=0.321, reward_mean=0.330, reward_bound=0.321, batch=227\n",
      "3200: loss=0.320, reward_mean=0.410, reward_bound=0.387, batch=228\n",
      "3300: loss=0.310, reward_mean=0.430, reward_bound=0.451, batch=231\n",
      "3400: loss=0.266, reward_mean=0.440, reward_bound=0.047, batch=180\n",
      "3500: loss=0.264, reward_mean=0.390, reward_bound=0.321, batch=227\n",
      "3600: loss=0.305, reward_mean=0.360, reward_bound=0.282, batch=207\n",
      "3700: loss=0.289, reward_mean=0.280, reward_bound=0.234, batch=228\n",
      "3800: loss=0.285, reward_mean=0.360, reward_bound=0.282, batch=216\n",
      "3900: loss=0.288, reward_mean=0.330, reward_bound=0.353, batch=229\n",
      "4000: loss=0.207, reward_mean=0.460, reward_bound=0.353, batch=229\n",
      "4100: loss=0.193, reward_mean=0.520, reward_bound=0.282, batch=219\n",
      "4200: loss=0.181, reward_mean=0.360, reward_bound=0.150, batch=204\n",
      "4300: loss=0.181, reward_mean=0.440, reward_bound=0.329, batch=227\n",
      "4400: loss=0.168, reward_mean=0.430, reward_bound=0.311, batch=226\n",
      "4600: loss=0.133, reward_mean=0.440, reward_bound=0.229, batch=216\n",
      "4700: loss=0.152, reward_mean=0.450, reward_bound=0.202, batch=215\n",
      "4800: loss=0.192, reward_mean=0.370, reward_bound=0.229, batch=218\n",
      "4900: loss=0.175, reward_mean=0.410, reward_bound=0.338, batch=224\n",
      "5000: loss=0.166, reward_mean=0.420, reward_bound=0.430, batch=223\n",
      "5100: loss=0.167, reward_mean=0.380, reward_bound=0.349, batch=213\n",
      "5200: loss=0.171, reward_mean=0.260, reward_bound=0.254, batch=216\n",
      "5300: loss=0.166, reward_mean=0.350, reward_bound=0.314, batch=182\n",
      "5400: loss=0.152, reward_mean=0.400, reward_bound=0.314, batch=224\n",
      "5500: loss=0.169, reward_mean=0.370, reward_bound=0.396, batch=227\n",
      "5600: loss=0.172, reward_mean=0.320, reward_bound=0.308, batch=222\n",
      "5700: loss=0.151, reward_mean=0.380, reward_bound=0.187, batch=215\n",
      "5800: loss=0.174, reward_mean=0.380, reward_bound=0.324, batch=218\n",
      "5900: loss=0.198, reward_mean=0.350, reward_bound=0.206, batch=221\n",
      "6000: loss=0.189, reward_mean=0.300, reward_bound=0.282, batch=224\n",
      "6100: loss=0.173, reward_mean=0.360, reward_bound=0.138, batch=213\n",
      "6200: loss=0.190, reward_mean=0.440, reward_bound=0.478, batch=227\n",
      "6300: loss=0.151, reward_mean=0.380, reward_bound=0.282, batch=217\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-66252c7ed308>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mfull_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0miter_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterate_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0mreward_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mfull_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_bound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_batch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPERCENTILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-66252c7ed308>\u001b[0m in \u001b[0;36miterate_batches\u001b[0;34m(env, net, batch_size)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mact_probs_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mact_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mact_probs_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mact_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mnext_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mepisode_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "import gym.spaces\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 100 #Larger batches of played episodes\n",
    "PERCENTILE = 30\n",
    "GAMMA = 0.9 #Discount factor applied to reward\n",
    "\n",
    "\n",
    "class DiscreteOneHotWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(DiscreteOneHotWrapper, self).__init__(env)\n",
    "        assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
    "        self.observation_space = gym.spaces.Box(0.0, 1.0, (env.observation_space.n, ), dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        res = np.copy(self.observation_space.low)\n",
    "        res[observation] = 1.0\n",
    "        return res\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])\n",
    "\n",
    "\n",
    "def iterate_batches(env, net, batch_size):\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    obs = env.reset()\n",
    "    sm = nn.Softmax(dim=1)\n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor([obs])\n",
    "        act_probs_v = sm(net(obs_v))\n",
    "        act_probs = act_probs_v.data.numpy()[0]\n",
    "        action = np.random.choice(len(act_probs), p=act_probs)\n",
    "        next_obs, reward, is_done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        episode_steps.append(EpisodeStep(observation=obs, action=action))\n",
    "        if is_done:\n",
    "            batch.append(Episode(reward=episode_reward, steps=episode_steps))\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            next_obs = env.reset()\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        obs = next_obs\n",
    "\n",
    "\n",
    "def filter_batch(batch, percentile):\n",
    "    disc_rewards = list(map(lambda s: s.reward * (GAMMA ** len(s.steps)), batch)) #discount factor\n",
    "    reward_bound = np.percentile(disc_rewards, percentile)\n",
    "\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    elite_batch = []\n",
    "    for example, discounted_reward in zip(batch, disc_rewards):\n",
    "        if discounted_reward > reward_bound:\n",
    "            train_obs.extend(map(lambda step: step.observation, example.steps))\n",
    "            train_act.extend(map(lambda step: step.action, example.steps))\n",
    "            elite_batch.append(example)\n",
    "\n",
    "    return elite_batch, train_obs, train_act, reward_bound # elite_batch\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    random.seed(12345)\n",
    "    env = DiscreteOneHotWrapper(gym.make(\"FrozenLake-v0\"))\n",
    "    # env = gym.wrappers.Monitor(env, directory=\"mon\", force=True)\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "    objective = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(params=net.parameters(), lr=0.001) #the learning rate decreased 10 times\n",
    "    writer = SummaryWriter(comment=\"-frozenlake-tweaked\")\n",
    "\n",
    "    full_batch = []\n",
    "    for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
    "        reward_mean = float(np.mean(list(map(lambda s: s.reward, batch))))\n",
    "        # use previous \"elite\" episodes \n",
    "        full_batch, obs, acts, reward_bound = filter_batch(full_batch + batch, PERCENTILE)\n",
    "        if not full_batch:\n",
    "            continue\n",
    "        obs_v = torch.FloatTensor(obs)\n",
    "        acts_v = torch.LongTensor(acts)\n",
    "        full_batch = full_batch[-500:]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        action_scores_v = net(obs_v)\n",
    "        loss_v = objective(action_scores_v, acts_v)\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "        if iter_no%100==0:\n",
    "            print(\"%d: loss=%.3f, reward_mean=%.3f, reward_bound=%.3f, batch=%d\" % (\n",
    "            iter_no, loss_v.item(), reward_mean, reward_bound, len(full_batch)))\n",
    "        writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
    "        writer.add_scalar(\"reward_mean\", reward_mean, iter_no)\n",
    "        writer.add_scalar(\"reward_bound\", reward_bound, iter_no)\n",
    "        if reward_mean > 0.8:\n",
    "            print(\"Solved!\")\n",
    "            break\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### frozenlake nonslippery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T18:02:40.994093Z",
     "start_time": "2019-07-22T18:02:15.126264Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drdh/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Trying to monitor an environment which has no 'spec' set. This usually means you did not create it via 'gym.make', and is recommended only for advanced users.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10: loss=1.358, reward_mean=0.040, reward_bound=0.000, batch=16\n",
      "20: loss=1.308, reward_mean=0.020, reward_bound=0.000, batch=34\n",
      "30: loss=1.265, reward_mean=0.070, reward_bound=0.000, batch=71\n",
      "40: loss=1.228, reward_mean=0.060, reward_bound=0.000, batch=140\n",
      "50: loss=1.171, reward_mean=0.120, reward_bound=0.122, batch=220\n",
      "60: loss=1.013, reward_mean=0.190, reward_bound=0.314, batch=205\n",
      "70: loss=0.849, reward_mean=0.240, reward_bound=0.387, batch=213\n",
      "80: loss=0.690, reward_mean=0.330, reward_bound=0.328, batch=181\n",
      "90: loss=0.696, reward_mean=0.440, reward_bound=0.000, batch=86\n",
      "100: loss=0.557, reward_mean=0.560, reward_bound=0.387, batch=135\n",
      "110: loss=0.404, reward_mean=0.600, reward_bound=0.430, batch=99\n",
      "Solved!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "import gym.spaces\n",
    "import gym.wrappers\n",
    "import gym.envs.toy_text.frozen_lake\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 100\n",
    "PERCENTILE = 30\n",
    "GAMMA = 0.9\n",
    "\n",
    "\n",
    "class DiscreteOneHotWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(DiscreteOneHotWrapper, self).__init__(env)\n",
    "        assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
    "        self.observation_space = gym.spaces.Box(0.0, 1.0, (env.observation_space.n, ), dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        res = np.copy(self.observation_space.low)\n",
    "        res[observation] = 1.0\n",
    "        return res\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])\n",
    "\n",
    "\n",
    "def iterate_batches(env, net, batch_size):\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    obs = env.reset()\n",
    "    sm = nn.Softmax(dim=1)\n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor([obs])\n",
    "        act_probs_v = sm(net(obs_v))\n",
    "        act_probs = act_probs_v.data.numpy()[0]\n",
    "        action = np.random.choice(len(act_probs), p=act_probs)\n",
    "        next_obs, reward, is_done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        episode_steps.append(EpisodeStep(observation=obs, action=action))\n",
    "        if is_done:\n",
    "            batch.append(Episode(reward=episode_reward, steps=episode_steps))\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            next_obs = env.reset()\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        obs = next_obs\n",
    "\n",
    "\n",
    "def filter_batch(batch, percentile):\n",
    "    disc_rewards = list(map(lambda s: s.reward * (GAMMA ** len(s.steps)), batch))\n",
    "    reward_bound = np.percentile(disc_rewards, percentile)\n",
    "\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    elite_batch = []\n",
    "    for example, discounted_reward in zip(batch, disc_rewards):\n",
    "        if discounted_reward > reward_bound:\n",
    "            train_obs.extend(map(lambda step: step.observation, example.steps))\n",
    "            train_act.extend(map(lambda step: step.action, example.steps))\n",
    "            elite_batch.append(example)\n",
    "\n",
    "    return elite_batch, train_obs, train_act, reward_bound\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    random.seed(12345)\n",
    "    env = gym.envs.toy_text.frozen_lake.FrozenLakeEnv(is_slippery=False)\n",
    "    #env = gym.wrappers.TimeLimit(env, max_episode_steps=100) # env.spec is NoneType,error\n",
    "    env = DiscreteOneHotWrapper(env)\n",
    "    env = gym.wrappers.Monitor(env, directory=\"mon\", force=True)\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "    objective = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(params=net.parameters(), lr=0.001)\n",
    "    writer = SummaryWriter(comment=\"-frozenlake-nonslippery\")\n",
    "\n",
    "    full_batch = []\n",
    "    for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
    "        reward_mean = float(np.mean(list(map(lambda s: s.reward, batch))))\n",
    "        full_batch, obs, acts, reward_bound = filter_batch(full_batch + batch, PERCENTILE)\n",
    "        if not full_batch:\n",
    "            continue\n",
    "        obs_v = torch.FloatTensor(obs)\n",
    "        acts_v = torch.LongTensor(acts)\n",
    "        full_batch = full_batch[-500:]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        action_scores_v = net(obs_v)\n",
    "        loss_v = objective(action_scores_v, acts_v)\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "        if iter_no%10==0:\n",
    "            print(\"%d: loss=%.3f, reward_mean=%.3f, reward_bound=%.3f, batch=%d\" % (\n",
    "            iter_no, loss_v.item(), reward_mean, reward_bound, len(full_batch)))\n",
    "        writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
    "        writer.add_scalar(\"reward_mean\", reward_mean, iter_no)\n",
    "        writer.add_scalar(\"reward_bound\", reward_bound, iter_no)\n",
    "        if reward_mean > 0.8:\n",
    "            print(\"Solved!\")\n",
    "            break\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Theoretical background of the cross-entropy method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The basis of the cross-entropy method lies in the importance sampling theorem, which states this:\n",
    "$$\n",
    "\\mathbb{E}_{x\\sim p(x)}[H(x)]=\\mathbb{E}_{x\\sim q(x)}\\left[\\frac{p(x)}{q(x)}H(x)  \\right]\n",
    "$$\n",
    "In our RL case, $H(x)$ is a reward value obtained by some policy $x$ and $p(x)$ is a distribution of all possible policies. We don't want to maximize our reward by searching all possible policies, instead we want to find a way to approximate $p(x)H(x)$ by $q(x)$, iteratively minimizing the distance between them. The distance between two probability distributions is calculated by KL-divergence which is as follows:\n",
    "\n",
    "$$\n",
    "KL(p_1(x)||p_2(x))=\\mathbb{E}_{x\\sim p_1(x)}\\log \\frac{p_1(x)}{p_2(x)} = \\mathbb{E}_{x\\sim p_1(x)}{[\\log p_1(x)]}-\\mathbb{E}_{x\\sim p_1(x)}{[\\log p_2(x)]}\n",
    "$$\n",
    "\n",
    "The first term in KL is called **entropy** and doesn't depend on that, so could be\n",
    "omitted during the minimization. The second term is called **cross-entropy** and is\n",
    "a very common optimization objective in DL.\n",
    "\n",
    "Combining both formulas, we can get an iterative algorithm, which starts with $q_0(x)=p(x)$ and on every step improves. This is an approximation of $p(x)H(x)$ with an update:\n",
    "\n",
    "$$\n",
    "q_{i+1}(x)=\\arg\\min_{q_{i+1}(x)}-\\mathbb{E}_{x\\sim q_i (x)}\\frac{p(x)}{q_i(x)}H(x)\\log q_{i+1}(x)\n",
    "$$\n",
    "\n",
    "This is a generic cross-entropy method, which can be significantly simplified in\n",
    "our RL case. Firstly, we replace our $H(x)$ with an indicator function, which is 1\n",
    "when the reward for the episode is above the threshold and 0 if the reward is\n",
    "below. Our policy update will look like this:\n",
    "\n",
    "$$\n",
    "\\pi_{i+1}(a|s)=\\arg\\min_{\\pi_{i+1}(a|s)}-\\mathbb{E}_{z\\sim \\pi_{i+1}(a|s)}[R(z)\\ge \\psi_i]\\log \\pi_{i+1}(a|s)\n",
    "$$\n",
    "\n",
    "Strictly speaking, the preceding formula misses the normalization term, but it\n",
    "still works in practice without it. So, the method is quite clear: we sample\n",
    "episodes using our current policy (starting with some random initial policy) and\n",
    "minimize the negative log likelihood of the most successful samples and our\n",
    "policy.\n",
    "\n",
    "There is a whole book dedicated to this method, written by Dirk P. Kroese. A\n",
    "shorter description can be found in the Cross-Entropy Method paper by Dirk\n",
    "P.Kroese (https://people.smp.uq.edu.au/DirkKroese/ps/eormsCE.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T15:38:17.504633Z",
     "start_time": "2019-07-21T15:38:17.501013Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "## Tabular Learning and the Bellman Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "the value of the state is:\n",
    "$$\n",
    "V(s)=\\mathbb{E}\\left[\\sum_{t=0}^\\infty \\gamma^t r_t  \\right]\n",
    "$$\n",
    "\n",
    "Bellman equation:\n",
    "$$\n",
    "V_0(a)=\\mathbb{E}_{s\\sim S}[r_{s,a}+\\gamma V_s]\n",
    "$$\n",
    "\n",
    "Bellman optimality equation\n",
    "$$\n",
    "V_0=\\max_{a\\in A}\\mathbb{E}_{s\\sim S}[r_{s,a}+\\gamma V_s]\n",
    "$$\n",
    "\n",
    "the value of state: $V_s$, the value of action $Q_{s,a}$, the latter is more convenient in practice\n",
    "$$\n",
    "Q_{s,a}=\\mathbb{E}_{s'\\sim S}[r_{s,a}+\\gamma V_{s'}]\n",
    "$$\n",
    "relatation between Q and V \n",
    "$$\n",
    "V_s=\\max_{a\\in A}Q_{s,a}\n",
    "$$\n",
    "So, replace V with Q in the Q function, we have:\n",
    "$$\n",
    "Q_{s,a}=\\mathbb{E}_{s'\\sim S}[r_{s,a}+\\gamma \\max_{a'\\in A}Q_{s',a'}]\n",
    "$$\n",
    "or simply:\n",
    "$$\n",
    "Q_{s,a}=r_{s,a}+\\gamma \\max_{a'\\in A}Q_{s',a'}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### frozenlake v-iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "for values of **states**:\n",
    "1. Initialize values of all states $V(s)$ to some initial value (usually zero)\n",
    "2. For every state $s$ in the MDPs, perform the Bellman update:\n",
    "    $$\n",
    "    V(s) \\leftarrow \\max_a \\left( r(s,a)+\\gamma \\mathbb{E}_{s'\\sim p(s'|s,a)}[V(s']\\right)\n",
    "    $$\n",
    "3. Repeat step2 for some large number of steps or until changes become too small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "obvious limitations:\n",
    "1. state space should be discrete and small\n",
    "2. transition probability and rewards matrix are needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "the calculation of the state action values\n",
    "\n",
    "```python\n",
    "def calc_action_value(self, state, action):\n",
    "```\n",
    "\n",
    "![](img/05_state_action_value.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T05:23:09.652330Z",
     "start_time": "2019-07-23T05:23:09.507830Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best reward updated 0.000 -> 0.150\n",
      "Best reward updated 0.150 -> 0.300\n",
      "Best reward updated 0.300 -> 0.400\n",
      "Best reward updated 0.400 -> 0.850\n",
      "Solved in 15 iterations!\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import collections\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "ENV_NAME = \"FrozenLake-v0\"\n",
    "GAMMA = 0.9\n",
    "TEST_EPISODES = 20\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(ENV_NAME)\n",
    "        self.state = self.env.reset()\n",
    "        self.rewards = collections.defaultdict(float) #reward table: (source,action,target)->reward\n",
    "        self.transits = collections.defaultdict(collections.Counter)#transition: (state,action)->{target:times}\n",
    "        self.values = collections.defaultdict(float)#value table: (state)->value\n",
    "\n",
    "    def play_n_random_steps(self, count):\n",
    "        for _ in range(count):\n",
    "            action = self.env.action_space.sample() #random\n",
    "            new_state, reward, is_done, _ = self.env.step(action)\n",
    "            self.rewards[(self.state, action, new_state)] = reward # reward table\n",
    "            self.transits[(self.state, action)][new_state] += 1 # transition table\n",
    "            self.state = self.env.reset() if is_done else new_state\n",
    "\n",
    "    def calc_action_value(self, state, action): # Q(s,a)\n",
    "        target_counts = self.transits[(state, action)]\n",
    "        total = sum(target_counts.values())\n",
    "        action_value = 0.0\n",
    "        for tgt_state, count in target_counts.items():\n",
    "            reward = self.rewards[(state, action, tgt_state)]\n",
    "            action_value += (count / total) * (reward + GAMMA * self.values[tgt_state])\n",
    "        return action_value\n",
    "\n",
    "    def select_action(self, state): #arg max_a Q(s,a)\n",
    "        best_action, best_value = None, None\n",
    "        for action in range(self.env.action_space.n):\n",
    "            action_value = self.calc_action_value(state, action)\n",
    "            if best_value is None or best_value < action_value:\n",
    "                best_value = action_value\n",
    "                best_action = action\n",
    "        return best_action\n",
    "\n",
    "    def play_episode(self, env):\n",
    "        total_reward = 0.0\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            action = self.select_action(state) #greedy\n",
    "            new_state, reward, is_done, _ = env.step(action)\n",
    "            self.rewards[(state, action, new_state)] = reward #also update \n",
    "            self.transits[(state, action)][new_state] += 1\n",
    "            total_reward += reward\n",
    "            if is_done:\n",
    "                break\n",
    "            state = new_state\n",
    "        return total_reward\n",
    "\n",
    "    def value_iteration(self):\n",
    "        for state in range(self.env.observation_space.n): #loop over all states\n",
    "            state_values = [self.calc_action_value(state, action) # all possible action and values\n",
    "                            for action in range(self.env.action_space.n)]\n",
    "            self.values[state] = max(state_values) # the max\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_env = gym.make(ENV_NAME)\n",
    "    agent = Agent()\n",
    "    writer = SummaryWriter(comment=\"-v-iteration\")\n",
    "\n",
    "    iter_no = 0\n",
    "    best_reward = 0.0\n",
    "    while True:\n",
    "        iter_no += 1\n",
    "        agent.play_n_random_steps(100) # random plays, populating the reward and transition tables.\n",
    "        agent.value_iteration() #update value table\n",
    "\n",
    "        reward = 0.0\n",
    "        for _ in range(TEST_EPISODES): #test using the updated value table\n",
    "            reward += agent.play_episode(test_env)\n",
    "        reward /= TEST_EPISODES\n",
    "        writer.add_scalar(\"reward\", reward, iter_no)\n",
    "        if reward > best_reward:\n",
    "            print(\"Best reward updated %.3f -> %.3f\" % (best_reward, reward))\n",
    "            best_reward = reward\n",
    "        if reward > 0.80:\n",
    "            print(\"Solved in %d iterations!\" % iter_no)\n",
    "            break\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### frozenlake q-iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "for values of **actions**:\n",
    "1. Initialize all $Q(s,a)$ to zero\n",
    "2. For every state $s$ and every action $a$ in this state, perform update:\n",
    "    $$\n",
    "    Q(s,a)\\leftarrow r(s,a)+\\gamma\\mathbb{E}_{s'\\sim p(s'|s,a)} \\left[\\max_{a'}Q(s',a') \\right]\n",
    "    $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T05:35:05.806548Z",
     "start_time": "2019-07-23T05:35:05.680033Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best reward updated 0.000 -> 0.650\n",
      "Best reward updated 0.650 -> 0.850\n",
      "Solved in 18 iterations!\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import collections\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "ENV_NAME = \"FrozenLake-v0\"\n",
    "GAMMA = 0.9\n",
    "TEST_EPISODES = 20\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(ENV_NAME)\n",
    "        self.state = self.env.reset()\n",
    "        self.rewards = collections.defaultdict(float)\n",
    "        self.transits = collections.defaultdict(collections.Counter)\n",
    "        self.values = collections.defaultdict(float) #Q(s,a) instead of V(s)\n",
    "\n",
    "    def play_n_random_steps(self, count):\n",
    "        for _ in range(count):\n",
    "            action = self.env.action_space.sample()\n",
    "            new_state, reward, is_done, _ = self.env.step(action)\n",
    "            self.rewards[(self.state, action, new_state)] = reward\n",
    "            self.transits[(self.state, action)][new_state] += 1\n",
    "            self.state = self.env.reset() if is_done else new_state\n",
    "\n",
    "    def select_action(self, state):\n",
    "        best_action, best_value = None, None\n",
    "        for action in range(self.env.action_space.n):\n",
    "            action_value = self.values[(state, action)]\n",
    "            if best_value is None or best_value < action_value:\n",
    "                best_value = action_value\n",
    "                best_action = action\n",
    "        return best_action\n",
    "\n",
    "    def play_episode(self, env):\n",
    "        total_reward = 0.0\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            action = self.select_action(state)\n",
    "            new_state, reward, is_done, _ = env.step(action)\n",
    "            self.rewards[(state, action, new_state)] = reward\n",
    "            self.transits[(state, action)][new_state] += 1\n",
    "            total_reward += reward\n",
    "            if is_done:\n",
    "                break\n",
    "            state = new_state\n",
    "        return total_reward\n",
    "\n",
    "    def value_iteration(self):\n",
    "        for state in range(self.env.observation_space.n): # s\n",
    "            for action in range(self.env.action_space.n): # a\n",
    "                action_value = 0.0\n",
    "                target_counts = self.transits[(state, action)] #(s,a)->(s'-> times)\n",
    "                total = sum(target_counts.values())\n",
    "                for tgt_state, count in target_counts.items():\n",
    "                    reward = self.rewards[(state, action, tgt_state)]\n",
    "                    best_action = self.select_action(tgt_state)\n",
    "                    action_value += (count / total) * (reward + GAMMA * self.values[(tgt_state, best_action)])\n",
    "                self.values[(state, action)] = action_value\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_env = gym.make(ENV_NAME)\n",
    "    agent = Agent()\n",
    "    writer = SummaryWriter(comment=\"-q-iteration\")\n",
    "\n",
    "    iter_no = 0\n",
    "    best_reward = 0.0\n",
    "    while True:\n",
    "        iter_no += 1\n",
    "        agent.play_n_random_steps(100)\n",
    "        agent.value_iteration()\n",
    "\n",
    "        reward = 0.0\n",
    "        for _ in range(TEST_EPISODES):\n",
    "            reward += agent.play_episode(test_env)\n",
    "        reward /= TEST_EPISODES\n",
    "        writer.add_scalar(\"reward\", reward, iter_no)\n",
    "        if reward > best_reward:\n",
    "            print(\"Best reward updated %.3f -> %.3f\" % (best_reward, reward))\n",
    "            best_reward = reward\n",
    "        if reward > 0.80:\n",
    "            print(\"Solved in %d iterations!\" % iter_no)\n",
    "            break\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### **Common Procedures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T08:46:23.158457Z",
     "start_time": "2019-07-23T08:46:23.152488Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#dqn_model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        return self.fc(conv_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T08:46:23.899270Z",
     "start_time": "2019-07-23T08:46:23.886135Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#wrappers\n",
    "import cv2\n",
    "import gym\n",
    "import gym.spaces\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "\n",
    "\n",
    "class FireResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None):\n",
    "        \"\"\"For environments where the user need to press FIRE for the game to start.\"\"\"\n",
    "        super(FireResetEnv, self).__init__(env)\n",
    "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "\n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        obs, _, done, _ = self.env.step(1)\n",
    "        if done:\n",
    "            self.env.reset()\n",
    "        obs, _, done, _ = self.env.step(2)\n",
    "        if done:\n",
    "            self.env.reset()\n",
    "        return obs\n",
    "\n",
    "\n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None, skip=4):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super(MaxAndSkipEnv, self).__init__(env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = collections.deque(maxlen=2)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for _ in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            self._obs_buffer.append(obs)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Clear past frame buffer and init. to first obs. from inner env.\"\"\"\n",
    "        self._obs_buffer.clear()\n",
    "        obs = self.env.reset()\n",
    "        self._obs_buffer.append(obs)\n",
    "        return obs\n",
    "\n",
    "\n",
    "class ProcessFrame84(gym.ObservationWrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(ProcessFrame84, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        return ProcessFrame84.process(obs)\n",
    "\n",
    "    @staticmethod\n",
    "    def process(frame):\n",
    "        if frame.size == 210 * 160 * 3:\n",
    "            img = np.reshape(frame, [210, 160, 3]).astype(np.float32)\n",
    "        elif frame.size == 250 * 160 * 3:\n",
    "            img = np.reshape(frame, [250, 160, 3]).astype(np.float32)\n",
    "        else:\n",
    "            assert False, \"Unknown resolution.\"\n",
    "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
    "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
    "        x_t = resized_screen[18:102, :]\n",
    "        x_t = np.reshape(x_t, [84, 84, 1])\n",
    "        return x_t.astype(np.uint8)\n",
    "\n",
    "\n",
    "class ImageToPyTorch(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(ImageToPyTorch, self).__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]),\n",
    "                                                dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.moveaxis(observation, 2, 0)\n",
    "\n",
    "\n",
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    def observation(self, obs):\n",
    "        return np.array(obs).astype(np.float32) / 255.0\n",
    "\n",
    "\n",
    "class BufferWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, n_steps, dtype=np.float32):\n",
    "        super(BufferWrapper, self).__init__(env)\n",
    "        self.dtype = dtype\n",
    "        old_space = env.observation_space\n",
    "        self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, axis=0),\n",
    "                                                old_space.high.repeat(n_steps, axis=0), dtype=dtype)\n",
    "\n",
    "    def reset(self):\n",
    "        self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)\n",
    "        return self.observation(self.env.reset())\n",
    "\n",
    "    def observation(self, observation):\n",
    "        self.buffer[:-1] = self.buffer[1:]\n",
    "        self.buffer[-1] = observation\n",
    "        return self.buffer\n",
    "\n",
    "def make_env(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    env = MaxAndSkipEnv(env)\n",
    "    env = FireResetEnv(env)\n",
    "    env = ProcessFrame84(env)\n",
    "    env = ImageToPyTorch(env)\n",
    "    env = BufferWrapper(env, 4)\n",
    "    return ScaledFloatFrame(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### frozenlake q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T05:44:18.680187Z",
     "start_time": "2019-07-23T05:44:09.312177Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best reward updated 0.000 -> 0.050\n",
      "Best reward updated 0.050 -> 0.100\n",
      "Best reward updated 0.100 -> 0.150\n",
      "Best reward updated 0.150 -> 0.200\n",
      "Best reward updated 0.200 -> 0.300\n",
      "Best reward updated 0.300 -> 0.350\n",
      "Best reward updated 0.350 -> 0.800\n",
      "Best reward updated 0.800 -> 0.850\n",
      "Solved in 2855 iterations!\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import collections\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "ENV_NAME = \"FrozenLake-v0\"\n",
    "GAMMA = 0.9\n",
    "ALPHA = 0.2\n",
    "TEST_EPISODES = 20\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(ENV_NAME)\n",
    "        self.state = self.env.reset()\n",
    "        self.values = collections.defaultdict(float)\n",
    "\n",
    "    def sample_env(self):\n",
    "        action = self.env.action_space.sample()\n",
    "        old_state = self.state\n",
    "        new_state, reward, is_done, _ = self.env.step(action)\n",
    "        self.state = self.env.reset() if is_done else new_state\n",
    "        return (old_state, action, reward, new_state)\n",
    "\n",
    "    def best_value_and_action(self, state):\n",
    "        best_value, best_action = None, None\n",
    "        for action in range(self.env.action_space.n):\n",
    "            action_value = self.values[(state, action)]\n",
    "            if best_value is None or best_value < action_value:\n",
    "                best_value = action_value\n",
    "                best_action = action\n",
    "        return best_value, best_action\n",
    "\n",
    "    def value_update(self, s, a, r, next_s):\n",
    "        best_v, _ = self.best_value_and_action(next_s)\n",
    "        new_val = r + GAMMA * best_v\n",
    "        old_val = self.values[(s, a)]\n",
    "        self.values[(s, a)] = old_val * (1-ALPHA) + new_val * ALPHA\n",
    "\n",
    "    def play_episode(self, env):\n",
    "        total_reward = 0.0\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            _, action = self.best_value_and_action(state)\n",
    "            new_state, reward, is_done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if is_done:\n",
    "                break\n",
    "            state = new_state\n",
    "        return total_reward\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_env = gym.make(ENV_NAME)\n",
    "    agent = Agent()\n",
    "    writer = SummaryWriter(comment=\"-q-learning\")\n",
    "\n",
    "    iter_no = 0\n",
    "    best_reward = 0.0\n",
    "    while True:\n",
    "        iter_no += 1\n",
    "        s, a, r, next_s = agent.sample_env()\n",
    "        agent.value_update(s, a, r, next_s)\n",
    "\n",
    "        reward = 0.0\n",
    "        for _ in range(TEST_EPISODES):\n",
    "            reward += agent.play_episode(test_env)\n",
    "        reward /= TEST_EPISODES\n",
    "        writer.add_scalar(\"reward\", reward, iter_no)\n",
    "        if reward > best_reward:\n",
    "            print(\"Best reward updated %.3f -> %.3f\" % (best_reward, reward))\n",
    "            best_reward = reward\n",
    "        if reward > 0.80:\n",
    "            print(\"Solved in %d iterations!\" % iter_no)\n",
    "            break\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dqn pong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-23T09:08:15.874342Z",
     "start_time": "2019-07-23T08:49:19.288320Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=6, bias=True)\n",
      "  )\n",
      ")\n",
      "1097: done 1 games, mean reward -19.000, eps 0.99, speed 858.11 f/s\n",
      "1986: done 2 games, mean reward -19.500, eps 0.98, speed 766.70 f/s\n",
      "2827: done 3 games, mean reward -20.000, eps 0.97, speed 839.71 f/s\n",
      "3722: done 4 games, mean reward -20.000, eps 0.96, speed 828.50 f/s\n",
      "4651: done 5 games, mean reward -20.200, eps 0.95, speed 792.24 f/s\n",
      "5487: done 6 games, mean reward -20.167, eps 0.95, speed 781.69 f/s\n",
      "6418: done 7 games, mean reward -20.143, eps 0.94, speed 747.11 f/s\n",
      "7320: done 8 games, mean reward -20.125, eps 0.93, speed 727.44 f/s\n",
      "8220: done 9 games, mean reward -20.111, eps 0.92, speed 725.58 f/s\n",
      "9130: done 10 games, mean reward -20.200, eps 0.91, speed 702.58 f/s\n",
      "9982: done 11 games, mean reward -20.273, eps 0.90, speed 664.70 f/s\n",
      "10976: done 12 games, mean reward -20.250, eps 0.89, speed 16.88 f/s\n",
      "11738: done 13 games, mean reward -20.308, eps 0.88, speed 15.69 f/s\n",
      "12709: done 14 games, mean reward -20.357, eps 0.87, speed 15.03 f/s\n",
      "13748: done 15 games, mean reward -20.333, eps 0.86, speed 14.73 f/s\n",
      "14733: done 16 games, mean reward -20.312, eps 0.85, speed 14.23 f/s\n",
      "15662: done 17 games, mean reward -20.294, eps 0.84, speed 13.45 f/s\n",
      "16474: done 18 games, mean reward -20.333, eps 0.84, speed 13.57 f/s\n",
      "17314: done 19 games, mean reward -20.316, eps 0.83, speed 13.73 f/s\n",
      "18304: done 20 games, mean reward -20.350, eps 0.82, speed 13.92 f/s\n",
      "19262: done 21 games, mean reward -20.381, eps 0.81, speed 14.18 f/s\n",
      "20086: done 22 games, mean reward -20.409, eps 0.80, speed 12.30 f/s\n",
      "21097: done 23 games, mean reward -20.435, eps 0.79, speed 11.80 f/s\n",
      "21921: done 24 games, mean reward -20.458, eps 0.78, speed 11.24 f/s\n",
      "23064: done 25 games, mean reward -20.400, eps 0.77, speed 12.54 f/s\n",
      "24038: done 26 games, mean reward -20.346, eps 0.76, speed 13.07 f/s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-79aa41ddba4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mloss_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0mloss_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#from lib import wrappers\n",
    "#from lib import dqn_model\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n",
    "MEAN_REWARD_BOUND = 19.5\n",
    "\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 32\n",
    "REPLAY_SIZE = 10000\n",
    "LEARNING_RATE = 1e-4\n",
    "SYNC_TARGET_FRAMES = 1000\n",
    "REPLAY_START_SIZE = 10000\n",
    "\n",
    "EPSILON_DECAY_LAST_FRAME = 10**5\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_FINAL = 0.02\n",
    "\n",
    "\n",
    "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
    "\n",
    "\n",
    "class ExperienceBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
    "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
    "               np.array(dones, dtype=np.uint8), np.array(next_states)\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env, exp_buffer):\n",
    "        self.env = env\n",
    "        self.exp_buffer = exp_buffer\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        self.state = env.reset()\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "    def play_step(self, net, epsilon=0.0, device=\"cpu\"):\n",
    "        done_reward = None\n",
    "\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            state_a = np.array([self.state], copy=False)\n",
    "            state_v = torch.tensor(state_a).to(device)\n",
    "            q_vals_v = net(state_v)\n",
    "            _, act_v = torch.max(q_vals_v, dim=1)\n",
    "            action = int(act_v.item())\n",
    "\n",
    "        # do step in the environment\n",
    "        new_state, reward, is_done, _ = self.env.step(action)\n",
    "        self.total_reward += reward\n",
    "\n",
    "        exp = Experience(self.state, action, reward, is_done, new_state)\n",
    "        self.exp_buffer.append(exp)\n",
    "        self.state = new_state\n",
    "        if is_done:\n",
    "            done_reward = self.total_reward\n",
    "            self._reset()\n",
    "        return done_reward\n",
    "\n",
    "\n",
    "def calc_loss(batch, net, tgt_net, device=\"cpu\"):\n",
    "    states, actions, rewards, dones, next_states = batch\n",
    "\n",
    "    states_v = torch.tensor(states).to(device)\n",
    "    next_states_v = torch.tensor(next_states).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    done_mask = torch.ByteTensor(dones).to(device)\n",
    "\n",
    "    state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "    next_state_values = tgt_net(next_states_v).max(1)[0]\n",
    "    next_state_values[done_mask] = 0.0\n",
    "    next_state_values = next_state_values.detach()\n",
    "\n",
    "    expected_state_action_values = next_state_values * GAMMA + rewards_v\n",
    "    return nn.MSELoss()(state_action_values, expected_state_action_values)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #parser = argparse.ArgumentParser()\n",
    "    #parser.add_argument(\"--cuda\", default=False, action=\"store_true\", help=\"Enable cuda\")\n",
    "    #parser.add_argument(\"--env\", default=DEFAULT_ENV_NAME,\n",
    "    #                    help=\"Name of the environment, default=\" + DEFAULT_ENV_NAME)\n",
    "    #parser.add_argument(\"--reward\", type=float, default=MEAN_REWARD_BOUND,\n",
    "    #                    help=\"Mean reward boundary for stop of training, default=%.2f\" % MEAN_REWARD_BOUND)\n",
    "    #args = parser.parse_args()\n",
    "    \n",
    "    #device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "    device=torch.device(\"cpu\") \n",
    "    \n",
    "    args_env=DEFAULT_ENV_NAME\n",
    "    args_reward=MEAN_REWARD_BOUND\n",
    "    \n",
    "    #env = wrappers.make_env(args.env) \n",
    "    env = make_env(args_env) \n",
    "\n",
    "    #net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "    net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "    #tgt_net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "    tgt_net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "    writer = SummaryWriter(comment=\"-\" + args_env)#args.env\n",
    "    print(net)\n",
    "\n",
    "    buffer = ExperienceBuffer(REPLAY_SIZE)\n",
    "    agent = Agent(env, buffer)\n",
    "    epsilon = EPSILON_START\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "    total_rewards = []\n",
    "    frame_idx = 0\n",
    "    ts_frame = 0\n",
    "    ts = time.time()\n",
    "    best_mean_reward = None\n",
    "\n",
    "    while True:\n",
    "        frame_idx += 1\n",
    "        epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)\n",
    "\n",
    "        reward = agent.play_step(net, epsilon, device=device)\n",
    "        if reward is not None:\n",
    "            total_rewards.append(reward)\n",
    "            speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
    "            ts_frame = frame_idx\n",
    "            ts = time.time()\n",
    "            mean_reward = np.mean(total_rewards[-100:])\n",
    "            print(\"%d: done %d games, mean reward %.3f, eps %.2f, speed %.2f f/s\" % (\n",
    "                frame_idx, len(total_rewards), mean_reward, epsilon,\n",
    "                speed\n",
    "            ))\n",
    "            writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
    "            writer.add_scalar(\"speed\", speed, frame_idx)\n",
    "            writer.add_scalar(\"reward_100\", mean_reward, frame_idx)\n",
    "            writer.add_scalar(\"reward\", reward, frame_idx)\n",
    "            if best_mean_reward is None or best_mean_reward < mean_reward:\n",
    "                #torch.save(net.state_dict(), args.env + \"-best.dat\")\n",
    "                torch.save(net.state_dict(), args_env + \"-best.dat\")\n",
    "                if best_mean_reward is not None:\n",
    "                    print(\"Best mean reward updated %.3f -> %.3f, model saved\" % (best_mean_reward, mean_reward))\n",
    "                best_mean_reward = mean_reward\n",
    "            #if mean_reward > args.reward:\n",
    "            if mean_reward > args_reward:\n",
    "                print(\"Solved in %d frames!\" % frame_idx)\n",
    "                break\n",
    "\n",
    "        if len(buffer) < REPLAY_START_SIZE:\n",
    "            continue\n",
    "\n",
    "        if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
    "            tgt_net.load_state_dict(net.state_dict())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch = buffer.sample(BATCH_SIZE)\n",
    "        loss_t = calc_loss(batch, net, tgt_net, device=device)\n",
    "        loss_t.backward()\n",
    "        optimizer.step()\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dqn play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from lib import wrappers\n",
    "#from lib import dqn_model\n",
    "\n",
    "import collections\n",
    "\n",
    "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n",
    "FPS = 25\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #parser = argparse.ArgumentParser()\n",
    "    #parser.add_argument(\"-m\", \"--model\", required=True, help=\"Model file to load\")\n",
    "    #parser.add_argument(\"-e\", \"--env\", default=DEFAULT_ENV_NAME,\n",
    "    #                    help=\"Environment name to use, default=\" + DEFAULT_ENV_NAME)\n",
    "    #parser.add_argument(\"-r\", \"--record\", help=\"Directory to store video recording\")\n",
    "    #parser.add_argument(\"--no-visualize\", default=True, action='store_false', dest='visualize',\n",
    "    #                    help=\"Disable visualization of the game play\")\n",
    "    #args = parser.parse_args()\n",
    "\n",
    "    #env = wrappers.make_env(args.env)\n",
    "    env = make_env(args.env)\n",
    "    args_record=False\n",
    "    args_env=DEFAULT_ENV_NAME\n",
    "    args_model=args_env + \"-best.dat\"\n",
    "    args_visualize=True\n",
    "    \n",
    "    #if args.record:\n",
    "    if args_record:\n",
    "        #env = gym.wrappers.Monitor(env, args.record)\n",
    "        env = gym.wrappers.Monitor(env, args_record)\n",
    "    #net = dqn_model.DQN(env.observation_space.shape, env.action_space.n)\n",
    "    net = DQN(env.observation_space.shape, env.action_space.n)\n",
    "    \n",
    "    #net.load_state_dict(torch.load(args.model, map_location=lambda storage, loc: storage))\n",
    "    net.load_state_dict(torch.load(args_model, map_location=lambda storage, loc: storage))\n",
    "\n",
    "    state = env.reset()\n",
    "    total_reward = 0.0\n",
    "    c = collections.Counter()\n",
    "\n",
    "    while True:\n",
    "        start_ts = time.time()\n",
    "        if args.visualize:\n",
    "            env.render()\n",
    "        state_v = torch.tensor(np.array([state], copy=False))\n",
    "        q_vals = net(state_v).data.numpy()[0]\n",
    "        action = np.argmax(q_vals)\n",
    "        c[action] += 1\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "        #if args.visualize:\n",
    "        if args_visualize:\n",
    "            delta = 1/FPS - (time.time() - start_ts)\n",
    "            if delta > 0:\n",
    "                time.sleep(delta)\n",
    "    print(\"Total reward: %.2f\" % total_reward)\n",
    "    print(\"Action counts:\", c)\n",
    "    #if args.record:\n",
    "    if args_record:\n",
    "        env.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stocks Trading Using RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradients -- An Alternative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Actor-Critic Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T15:40:36.997768Z",
     "start_time": "2019-07-21T15:40:36.994182Z"
    }
   },
   "source": [
    "## Asynchronous Advantage Actor-Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbots Training with RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Action Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T15:42:20.701409Z",
     "start_time": "2019-07-21T15:42:20.697313Z"
    }
   },
   "source": [
    "## Trust Regions -- TRPO,PPO and ACKTR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T15:42:55.115122Z",
     "start_time": "2019-07-21T15:42:55.111679Z"
    }
   },
   "source": [
    "## Black-Box Optimization in RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond Model-Free -- Imagination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlphaGo Zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "300px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
