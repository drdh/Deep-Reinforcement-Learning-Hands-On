{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T15:29:28.733243Z",
     "start_time": "2019-07-21T15:29:28.730839Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "## OpenAI Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### agent anatomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T15:49:38.486521Z",
     "start_time": "2019-07-21T15:49:38.475568Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward got: 3.5296\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.steps_left = 10\n",
    "\n",
    "    def get_observation(self):\n",
    "        return [0.0, 0.0, 0.0]\n",
    "\n",
    "    def get_actions(self):\n",
    "        return [0, 1]\n",
    "\n",
    "    def is_done(self):\n",
    "        return self.steps_left == 0\n",
    "\n",
    "    def action(self, action):\n",
    "        if self.is_done():\n",
    "            raise Exception(\"Game is over\")\n",
    "        self.steps_left -= 1\n",
    "        return random.random()\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "    def step(self, env):\n",
    "        current_obs = env.get_observation()\n",
    "        actions = env.get_actions()\n",
    "        reward = env.action(random.choice(actions))\n",
    "        self.total_reward += reward\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = Environment()\n",
    "    agent = Agent()\n",
    "\n",
    "    while not env.is_done():\n",
    "        agent.step(env)\n",
    "\n",
    "    print(\"Total reward got: %.4f\" % agent.total_reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### cartpole_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T15:58:25.857599Z",
     "start_time": "2019-07-21T15:58:25.850402Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode done in 42 steps, total reward 42.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drdh/anaconda3/envs/RL_hands_on_Env/lib/python3.7/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "    total_reward = 0.0\n",
    "    total_steps = 0\n",
    "    obs = env.reset() #reset the env and obtain the first observation\n",
    "\n",
    "    while True:\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        total_steps += 1\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print(\"Episode done in %d steps, total reward %.2f\" % (total_steps, total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### random_actionwrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T16:09:00.308519Z",
     "start_time": "2019-07-21T16:09:00.284598Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random!\n",
      "Random!\n",
      "Reward got: 10.00\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "\n",
    "class RandomActionWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env, epsilon=0.1):\n",
    "        super(RandomActionWrapper, self).__init__(env)\n",
    "        self.epsilon = epsilon #random ratio\n",
    "\n",
    "    def action(self, action): #action wrapper\n",
    "        if random.random() < self.epsilon:\n",
    "            print(\"Random!\")\n",
    "            return self.env.action_space.sample()\n",
    "        return action\n",
    "    \n",
    "    # def observation(self,obs):\n",
    "    # def reward(self,rew):\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = RandomActionWrapper(gym.make(\"CartPole-v0\"))\n",
    "\n",
    "    obs = env.reset()\n",
    "    total_reward = 0.0\n",
    "\n",
    "    while True:\n",
    "        obs, reward, done, _ = env.step(0) #same action 0\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print(\"Reward got: %.2f\" % total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### cartpole_random_monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If encounter error `attributeerror 'imagedata' object has no attribute 'data' gym`\n",
    "\n",
    "```bash\n",
    "pip install pyglet==1.3.2\n",
    "sudo apt-get install ffmpeg\n",
    "```\n",
    "\n",
    "will solve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T16:28:18.039519Z",
     "start_time": "2019-07-21T16:28:17.546548Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drdh/anaconda3/envs/RL_hands_on_Env/lib/python3.7/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode done in 12 steps, total reward 12.00\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    env = gym.wrappers.Monitor(env, \"recording\",force=True)# in ./recording/\n",
    "\n",
    "    total_reward = 0.0\n",
    "    total_steps = 0\n",
    "    obs = env.reset()\n",
    "\n",
    "    while True:\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        total_steps += 1\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print(\"Episode done in %d steps, total reward %.2f\" % (total_steps, total_reward))\n",
    "    env.close()\n",
    "    env.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Deep Learning with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[nn.module](https://pytorch.org/docs/stable/nn.html#module)\n",
    "\n",
    "All classes in the `torch.nn` packages inherit from the `nn.Module` base class.\n",
    "\n",
    "Some useful methods:\n",
    "\n",
    "- `parameters()`: A function that resturns iterator of all variables which require gradient computation(that is, module weights)\n",
    "- `zero_grad()`: initializes all gradients of all parameters to 0\n",
    "- `to(device)`: moves all module parameters to a given device(CPU/GPU)\n",
    "- `state_dict()`: returns the dictionary with all module parameters and is useful for model serialization\n",
    "- `load_state_dict()`: initializes the module with the state dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T05:00:50.496720Z",
     "start_time": "2019-07-22T05:00:50.487215Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OurModule(\n",
      "  (pipe): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=5, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=5, out_features=20, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=20, out_features=3, bias=True)\n",
      "    (5): Dropout(p=0.3)\n",
      "    (6): Softmax()\n",
      "  )\n",
      ")\n",
      "tensor([[0.4174, 0.4174, 0.1652],\n",
      "        [0.2642, 0.6622, 0.0735],\n",
      "        [0.2278, 0.7236, 0.0486]], grad_fn=<SoftmaxBackward>)\n",
      "Cuda's availability is False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class OurModule(nn.Module):\n",
    "    def __init__(self, num_inputs, num_classes, dropout_prob=0.3):\n",
    "        super(OurModule, self).__init__()\n",
    "        self.pipe = nn.Sequential(\n",
    "            nn.Linear(num_inputs, 5),\n",
    "            #torch.nn.Linear(in_features, out_features, bias=True)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, num_classes),\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Softmax(dim=1)\n",
    "            #along dim=1,since dim=0 is batch samples\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pipe(x)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    net = OurModule(num_inputs=2, num_classes=3)\n",
    "    print(net)\n",
    "    v = torch.FloatTensor([[2, 3],[3,4],[4,5]])#dim=0: batch, dim=1,input\n",
    "    out = net(v)\n",
    "    print(out)\n",
    "    print(\"Cuda's availability is %s\" % torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Data from cuda: %s\" % out.to('cuda'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T17:01:59.097811Z",
     "start_time": "2019-07-21T17:01:58.883837Z"
    },
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T08:19:42.813709Z",
     "start_time": "2019-07-22T08:19:42.689479Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    funcs = {\"sin\": math.sin, \"cos\": math.cos, \"tan\": math.tan}\n",
    "\n",
    "    for angle in range(-360, 360):\n",
    "        angle_rad = angle * math.pi / 180\n",
    "        for name, fun in funcs.items():\n",
    "            val = fun(angle_rad)\n",
    "            writer.add_scalar(name, val, angle)\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "then run \n",
    "```bash\n",
    "tensorboard --logdir runs --host localhost\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### atari_gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T08:51:09.281706Z",
     "start_time": "2019-07-22T08:37:40.085966Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Making new env: Breakout-v0\n",
      "INFO: Making new env: AirRaid-v0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drdh/anaconda3/envs/RL_hands_on_Env/lib/python3.7/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Making new env: Pong-v0\n",
      "INFO: Iter 100: gen_loss=4.538e+00, dis_loss=1.008e-01\n",
      "INFO: Iter 200: gen_loss=6.244e+00, dis_loss=6.144e-03\n",
      "INFO: Iter 300: gen_loss=6.966e+00, dis_loss=2.346e-03\n",
      "INFO: Iter 400: gen_loss=7.283e+00, dis_loss=2.378e-02\n",
      "INFO: Iter 500: gen_loss=6.235e+00, dis_loss=6.072e-02\n",
      "INFO: Iter 600: gen_loss=5.509e+00, dis_loss=2.121e-01\n",
      "INFO: Iter 700: gen_loss=5.555e+00, dis_loss=1.789e-02\n",
      "INFO: Iter 800: gen_loss=6.426e+00, dis_loss=6.950e-03\n",
      "INFO: Iter 900: gen_loss=6.289e+00, dis_loss=1.374e-01\n",
      "INFO: Iter 1000: gen_loss=5.316e+00, dis_loss=2.340e-02\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (30) : unknown error at /pytorch/aten/src/THC/THCGeneral.cpp:74",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-3fd63c29379f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mdis_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0miter_no\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mSAVE_IMAGE_EVERY_ITER\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fake\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_output_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"real\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/RL_hands_on_Env/lib/python3.7/site-packages/tensorboardX/writer.py\u001b[0m in \u001b[0;36madd_image\u001b[0;34m(self, tag, img_tensor, global_step)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mimg_tensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mUse\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mto\u001b[0m \u001b[0mprepare\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mgood\u001b[0m \u001b[0midea\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \"\"\"\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msnd_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \"\"\"Add audio data to summary.\n",
      "\u001b[0;32m~/anaconda3/envs/RL_hands_on_Env/lib/python3.7/site-packages/tensorboardX/summary.py\u001b[0m in \u001b[0;36mimage\u001b[0;34m(tag, tensor)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_clean_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'input tensor should be one of numpy.ndarray, torch.cuda.FloatTensor, torch.FloatTensor'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m4\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'input tensor should be 3 dimensional.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (30) : unknown error at /pytorch/aten/src/THC/THCGeneral.cpp:74"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import random\n",
    "import argparse\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import gym\n",
    "import gym.spaces\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "log = gym.logger\n",
    "log.set_level(gym.logger.INFO)\n",
    "\n",
    "LATENT_VECTOR_SIZE = 100\n",
    "DISCR_FILTERS = 64\n",
    "GENER_FILTERS = 64\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# dimension input image will be rescaled\n",
    "IMAGE_SIZE = 64\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "REPORT_EVERY_ITER = 100\n",
    "SAVE_IMAGE_EVERY_ITER = 1000\n",
    "\n",
    "\n",
    "class InputWrapper(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Preprocessing of input numpy array:\n",
    "    1. resize image into predefined size\n",
    "    2. move color channel axis to a first place\n",
    "    \"\"\"\n",
    "    def __init__(self, *args):\n",
    "        super(InputWrapper, self).__init__(*args)\n",
    "        assert isinstance(self.observation_space, gym.spaces.Box)\n",
    "        old_space = self.observation_space\n",
    "        self.observation_space = gym.spaces.Box(self.observation(old_space.low), self.observation(old_space.high),\n",
    "                                                dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        #1. resize image #(210,160) -> (64,64)\n",
    "        new_obs = cv2.resize(observation, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "        #2. transform (210, 160, 3) -> (3, 210, 160) i.e.(channels,height,width)\n",
    "        new_obs = np.moveaxis(new_obs, 2, 0)\n",
    "        return new_obs.astype(np.float32)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # this pipe converges image into the single number\n",
    "        self.conv_pipe = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape[0], out_channels=DISCR_FILTERS,\n",
    "                      kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=DISCR_FILTERS, out_channels=DISCR_FILTERS*2,\n",
    "                      kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(DISCR_FILTERS*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=DISCR_FILTERS * 2, out_channels=DISCR_FILTERS * 4,\n",
    "                      kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(DISCR_FILTERS * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=DISCR_FILTERS * 4, out_channels=DISCR_FILTERS * 8,\n",
    "                      kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(DISCR_FILTERS * 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=DISCR_FILTERS * 8, out_channels=1,\n",
    "                      kernel_size=4, stride=1, padding=0),\n",
    "            #real/fake probability\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv_pipe(x)\n",
    "        return conv_out.view(-1, 1).squeeze(dim=1)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, output_shape):\n",
    "        super(Generator, self).__init__()\n",
    "        # pipe deconvolves input vector into (3, 64, 64) image\n",
    "        self.pipe = nn.Sequential(\n",
    "            #input: lantent vector\n",
    "            #transposed convolution: deconvolution\n",
    "            nn.ConvTranspose2d(in_channels=LATENT_VECTOR_SIZE, out_channels=GENER_FILTERS * 8,\n",
    "                               kernel_size=4, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(GENER_FILTERS * 8),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=GENER_FILTERS * 8, out_channels=GENER_FILTERS * 4,\n",
    "                               kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(GENER_FILTERS * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=GENER_FILTERS * 4, out_channels=GENER_FILTERS * 2,\n",
    "                               kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(GENER_FILTERS * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=GENER_FILTERS * 2, out_channels=GENER_FILTERS,\n",
    "                               kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(GENER_FILTERS),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=GENER_FILTERS, out_channels=output_shape[0],\n",
    "                               kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pipe(x)\n",
    "\n",
    "\n",
    "    #infinitely samples\n",
    "def iterate_batches(envs, batch_size=BATCH_SIZE):\n",
    "    batch = [e.reset() for e in envs]\n",
    "    env_gen = iter(lambda: random.choice(envs), None)\n",
    "\n",
    "    while True: \n",
    "        e = next(env_gen)\n",
    "        obs, reward, is_done, _ = e.step(e.action_space.sample()) #play by random agent\n",
    "        if np.mean(obs) > 0.01:\n",
    "            batch.append(obs)\n",
    "        if len(batch) == batch_size:\n",
    "            # Normalising input between -1 to 1\n",
    "            batch_np = np.array(batch, dtype=np.float32) * 2.0 / 255.0 - 1.0\n",
    "            yield torch.tensor(batch_np,dtype=torch.float32)\n",
    "            batch.clear()\n",
    "        if is_done:\n",
    "            e.reset()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #parser = argparse.ArgumentParser()\n",
    "    #parser.add_argument(\"--cuda\", default=False, action='store_true', help=\"Enable cuda computation\")\n",
    "    #args = parser.parse_args()\n",
    "\n",
    "    #device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "    device=torch.device(\"cpu\")\n",
    "    envs = [InputWrapper(gym.make(name)) for name in ('Breakout-v0', 'AirRaid-v0', 'Pong-v0')]\n",
    "    input_shape = envs[0].observation_space.shape\n",
    "    \n",
    "    #2 nets\n",
    "    net_discr = Discriminator(input_shape=input_shape).to(device)\n",
    "    net_gener = Generator(output_shape=input_shape).to(device)\n",
    "\n",
    "    objective = nn.BCELoss()\n",
    "    gen_optimizer = optim.Adam(params=net_gener.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "    dis_optimizer = optim.Adam(params=net_discr.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    gen_losses = []\n",
    "    dis_losses = []\n",
    "    iter_no = 0\n",
    "\n",
    "    true_labels_v = torch.ones(BATCH_SIZE, dtype=torch.float32, device=device)\n",
    "    fake_labels_v = torch.zeros(BATCH_SIZE, dtype=torch.float32, device=device)\n",
    "\n",
    "    for batch_v in iterate_batches(envs):\n",
    "        # generate extra fake samples, input is 4D: batch, filters, x, y\n",
    "        gen_input_v = torch.FloatTensor(BATCH_SIZE, LATENT_VECTOR_SIZE, 1, 1).normal_(0, 1).to(device)\n",
    "        batch_v = batch_v.to(device)\n",
    "        gen_output_v = net_gener(gen_input_v)\n",
    "\n",
    "        # train discriminator\n",
    "        dis_optimizer.zero_grad()\n",
    "        dis_output_true_v = net_discr(batch_v) #true data samples\n",
    "        dis_output_fake_v = net_discr(gen_output_v.detach()) #generated samples,detach():a copy(), no gradient flow\n",
    "        dis_loss = objective(dis_output_true_v, true_labels_v) + objective(dis_output_fake_v, fake_labels_v)\n",
    "        dis_loss.backward()\n",
    "        dis_optimizer.step()\n",
    "        dis_losses.append(dis_loss.item())\n",
    "\n",
    "        # train generator\n",
    "        gen_optimizer.zero_grad()\n",
    "        dis_output_v = net_discr(gen_output_v)\n",
    "        gen_loss_v = objective(dis_output_v, true_labels_v)\n",
    "        gen_loss_v.backward()\n",
    "        gen_optimizer.step()\n",
    "        gen_losses.append(gen_loss_v.item())\n",
    "\n",
    "        #for tensorboard: tensorboard --logdir runs --host localhost\n",
    "        iter_no += 1\n",
    "        if iter_no % REPORT_EVERY_ITER == 0:\n",
    "            log.info(\"Iter %d: gen_loss=%.3e, dis_loss=%.3e\", iter_no, np.mean(gen_losses), np.mean(dis_losses))\n",
    "            writer.add_scalar(\"gen_loss\", np.mean(gen_losses), iter_no)\n",
    "            writer.add_scalar(\"dis_loss\", np.mean(dis_losses), iter_no)\n",
    "            gen_losses = []\n",
    "            dis_losses = []\n",
    "        if iter_no % SAVE_IMAGE_EVERY_ITER == 0:\n",
    "            writer.add_image(\"fake\", vutils.make_grid(gen_output_v.data[:64], normalize=True), iter_no)\n",
    "            writer.add_image(\"real\", vutils.make_grid(batch_v.data[:64], normalize=True), iter_no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T02:37:28.468404Z",
     "start_time": "2019-07-22T02:37:28.457250Z"
    },
    "hidden": true
   },
   "source": [
    "```bash\n",
    "tensorboard --logdir runs --host localhost\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Cross-Entropy Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. sample $A_1,\\cdots,A_N$ from $p(A)$\n",
    "2. evaluate $J(A_1),\\cdots,J(A_N)$\n",
    "3. pick the **elites** $A_{i_1},\\cdots,A_{i_M}$ with the highest value, where $M<N$\n",
    "4. refit $p(A)$ to the elites $A_{i_1},\\cdots,A_{i_M}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T15:54:58.070144Z",
     "start_time": "2019-07-22T15:54:39.971962Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=0.694, reward_mean=20.2, reward_bound=22.0\n",
      "1: loss=0.694, reward_mean=28.0, reward_bound=32.0\n",
      "2: loss=0.687, reward_mean=24.2, reward_bound=29.0\n",
      "3: loss=0.670, reward_mean=23.6, reward_bound=23.0\n",
      "4: loss=0.661, reward_mean=27.9, reward_bound=32.5\n",
      "5: loss=0.661, reward_mean=37.3, reward_bound=43.0\n",
      "6: loss=0.658, reward_mean=43.8, reward_bound=55.0\n",
      "7: loss=0.650, reward_mean=45.3, reward_bound=58.0\n",
      "8: loss=0.629, reward_mean=30.8, reward_bound=33.5\n",
      "9: loss=0.638, reward_mean=35.1, reward_bound=39.5\n",
      "10: loss=0.629, reward_mean=45.6, reward_bound=54.0\n",
      "11: loss=0.607, reward_mean=49.8, reward_bound=52.5\n",
      "12: loss=0.623, reward_mean=42.6, reward_bound=46.0\n",
      "13: loss=0.608, reward_mean=52.8, reward_bound=60.5\n",
      "14: loss=0.621, reward_mean=49.2, reward_bound=60.5\n",
      "15: loss=0.595, reward_mean=47.5, reward_bound=54.0\n",
      "16: loss=0.591, reward_mean=48.5, reward_bound=55.5\n",
      "17: loss=0.593, reward_mean=52.3, reward_bound=57.5\n",
      "18: loss=0.580, reward_mean=62.2, reward_bound=67.5\n",
      "19: loss=0.577, reward_mean=74.2, reward_bound=88.0\n",
      "20: loss=0.583, reward_mean=62.1, reward_bound=73.0\n",
      "21: loss=0.571, reward_mean=58.2, reward_bound=63.0\n",
      "22: loss=0.570, reward_mean=69.9, reward_bound=81.5\n",
      "23: loss=0.590, reward_mean=72.9, reward_bound=85.0\n",
      "24: loss=0.559, reward_mean=76.8, reward_bound=88.5\n",
      "25: loss=0.548, reward_mean=72.1, reward_bound=80.0\n",
      "26: loss=0.553, reward_mean=88.8, reward_bound=99.5\n",
      "27: loss=0.548, reward_mean=91.6, reward_bound=102.5\n",
      "28: loss=0.547, reward_mean=77.9, reward_bound=81.5\n",
      "29: loss=0.549, reward_mean=98.6, reward_bound=108.0\n",
      "30: loss=0.542, reward_mean=112.9, reward_bound=135.5\n",
      "31: loss=0.523, reward_mean=103.1, reward_bound=123.0\n",
      "32: loss=0.548, reward_mean=105.4, reward_bound=136.0\n",
      "33: loss=0.525, reward_mean=123.1, reward_bound=145.0\n",
      "34: loss=0.535, reward_mean=159.7, reward_bound=183.5\n",
      "35: loss=0.530, reward_mean=143.9, reward_bound=153.0\n",
      "36: loss=0.544, reward_mean=141.3, reward_bound=167.5\n",
      "37: loss=0.522, reward_mean=169.8, reward_bound=200.0\n",
      "38: loss=0.519, reward_mean=183.4, reward_bound=200.0\n",
      "39: loss=0.529, reward_mean=190.8, reward_bound=200.0\n",
      "40: loss=0.522, reward_mean=192.9, reward_bound=200.0\n",
      "41: loss=0.530, reward_mean=193.4, reward_bound=200.0\n",
      "42: loss=0.524, reward_mean=194.9, reward_bound=200.0\n",
      "43: loss=0.527, reward_mean=200.0, reward_bound=200.0\n",
      "Solved!\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "HIDDEN_SIZE = 128 #random\n",
    "BATCH_SIZE = 16\n",
    "PERCENTILE = 70 #only leave top 30% \n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)# probability distribution over action\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "#helper\n",
    "Episode = namedtuple('Episode', field_names=['reward', 'steps']) #for a single episode\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])#for a single step\n",
    "\n",
    "\n",
    "def iterate_batches(env, net, batch_size):\n",
    "    batch = [] # to accumulate batch\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    obs = env.reset()\n",
    "    sm = nn.Softmax(dim=1) #softmax layer\n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor([obs])# (4) -> (1,4), add batch axis\n",
    "        act_probs_v = sm(net(obs_v)) #raw action scores are fed to softmax function\n",
    "        act_probs = act_probs_v.data.numpy()[0] #unpack tensors from gradient track\n",
    "        action = np.random.choice(len(act_probs), p=act_probs) #int a=np.arange(len(...)),p=probability\n",
    "        next_obs, reward, is_done, _ = env.step(action)\n",
    "        episode_reward += reward #total reward of this episode\n",
    "        episode_steps.append(EpisodeStep(observation=obs, action=action))\n",
    "        if is_done:\n",
    "            batch.append(Episode(reward=episode_reward, steps=episode_steps))#an episode is over\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            next_obs = env.reset()\n",
    "            if len(batch) == batch_size:#number of episode == batch_size\n",
    "                yield batch\n",
    "                batch = []\n",
    "        obs = next_obs #current obs\n",
    "\n",
    "\n",
    "def filter_batch(batch, percentile):\n",
    "    rewards = list(map(lambda s: s.reward,  batch))# get every episode's reward\n",
    "    reward_bound = np.percentile(rewards, percentile) #boundary reward, to filter elite episode\n",
    "    reward_mean = float(np.mean(rewards))\n",
    "\n",
    "    train_obs = [] # long long observations\n",
    "    train_act = [] # long long actions\n",
    "    for example in batch:\n",
    "        if example.reward < reward_bound:\n",
    "            continue # filter\n",
    "        train_obs.extend(map(lambda step: step.observation, example.steps))\n",
    "        train_act.extend(map(lambda step: step.action, example.steps))\n",
    "\n",
    "    train_obs_v = torch.FloatTensor(train_obs)\n",
    "    train_act_v = torch.LongTensor(train_act)\n",
    "    return train_obs_v, train_act_v, reward_bound, reward_mean # the last two are used in TensorBoard\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    env = gym.wrappers.Monitor(env, directory=\"mon\", force=True) #video\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "    objective = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(params=net.parameters(), lr=0.01)\n",
    "    writer = SummaryWriter(comment=\"-cartpole\")\n",
    "\n",
    "    for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
    "        obs_v, acts_v, reward_b, reward_m = filter_batch(batch, PERCENTILE)\n",
    "        optimizer.zero_grad()\n",
    "        action_scores_v = net(obs_v)\n",
    "        loss_v = objective(action_scores_v, acts_v) # fit Net over action distribution\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "        print(\"%d: loss=%.3f, reward_mean=%.1f, reward_bound=%.1f\" % (\n",
    "            iter_no, loss_v.item(), reward_m, reward_b))\n",
    "        writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
    "        writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n",
    "        writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n",
    "        if reward_m > 199:\n",
    "            print(\"Solved!\")\n",
    "            break\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### frozenlake naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T16:27:14.245802Z",
     "start_time": "2019-07-22T16:27:14.240121Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Discrete(16), Discrete(4), 0, None)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "e=gym.make(\"FrozenLake-v0\")\n",
    "e.observation_space,e.action_space,e.reset(),e.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T16:50:44.577203Z",
     "start_time": "2019-07-22T16:50:43.820318Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=1.399, reward_mean=0.0, reward_bound=0.0\n",
      "1: loss=1.373, reward_mean=0.0, reward_bound=0.0\n",
      "2: loss=1.350, reward_mean=0.2, reward_bound=0.0\n",
      "3: loss=1.337, reward_mean=0.0, reward_bound=0.0\n",
      "4: loss=1.332, reward_mean=0.0, reward_bound=0.0\n",
      "5: loss=1.333, reward_mean=0.0, reward_bound=0.0\n",
      "6: loss=1.286, reward_mean=0.0, reward_bound=0.0\n",
      "7: loss=1.290, reward_mean=0.0, reward_bound=0.0\n",
      "8: loss=1.295, reward_mean=0.0, reward_bound=0.0\n",
      "9: loss=1.261, reward_mean=0.0, reward_bound=0.0\n",
      "10: loss=1.321, reward_mean=0.0, reward_bound=0.0\n",
      "11: loss=1.245, reward_mean=0.0, reward_bound=0.0\n",
      "12: loss=1.323, reward_mean=0.0, reward_bound=0.0\n",
      "13: loss=1.265, reward_mean=0.0, reward_bound=0.0\n",
      "14: loss=1.282, reward_mean=0.0, reward_bound=0.0\n",
      "15: loss=1.229, reward_mean=0.0, reward_bound=0.0\n",
      "16: loss=1.129, reward_mean=0.0, reward_bound=0.0\n",
      "17: loss=1.256, reward_mean=0.0, reward_bound=0.0\n",
      "18: loss=1.163, reward_mean=0.0, reward_bound=0.0\n",
      "19: loss=1.181, reward_mean=0.0, reward_bound=0.0\n",
      "20: loss=1.170, reward_mean=0.0, reward_bound=0.0\n",
      "21: loss=1.048, reward_mean=0.0, reward_bound=0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b538fac60e60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"-frozenlake-naive\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0miter_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterate_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0mobs_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macts_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPERCENTILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-b538fac60e60>\u001b[0m in \u001b[0;36miterate_batches\u001b[0;34m(env, net, batch_size)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mact_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mact_probs_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mact_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mnext_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mepisode_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mepisode_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEpisodeStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36m_after_step\u001b[0;34m(self, observation, reward, done, info)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;31m# Record stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0;31m# Record video\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/wrappers/monitoring/stats_recorder.py\u001b[0m in \u001b[0;36mafter_step\u001b[0;34m(self, observation, reward, done, info)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym, gym.spaces\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "PERCENTILE = 70\n",
    "\n",
    "\n",
    "class DiscreteOneHotWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(DiscreteOneHotWrapper, self).__init__(env)\n",
    "        assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
    "        self.observation_space = gym.spaces.Box(0.0, 1.0, (env.observation_space.n, ), dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        res = np.copy(self.observation_space.low)\n",
    "        res[observation] = 1.0\n",
    "        return res\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])\n",
    "\n",
    "\n",
    "def iterate_batches(env, net, batch_size):\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    obs = env.reset()\n",
    "    sm = nn.Softmax(dim=1)\n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor([obs])\n",
    "        act_probs_v = sm(net(obs_v))\n",
    "        act_probs = act_probs_v.data.numpy()[0]\n",
    "        action = np.random.choice(len(act_probs), p=act_probs)\n",
    "        next_obs, reward, is_done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        episode_steps.append(EpisodeStep(observation=obs, action=action))\n",
    "        if is_done:\n",
    "            batch.append(Episode(reward=episode_reward, steps=episode_steps))\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            next_obs = env.reset()\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        obs = next_obs\n",
    "\n",
    "\n",
    "def filter_batch(batch, percentile):\n",
    "    rewards = list(map(lambda s: s.reward, batch))\n",
    "    reward_bound = np.percentile(rewards, percentile)\n",
    "    reward_mean = float(np.mean(rewards))\n",
    "\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    for example in batch:\n",
    "        if example.reward < reward_bound:\n",
    "            continue\n",
    "        train_obs.extend(map(lambda step: step.observation, example.steps))\n",
    "        train_act.extend(map(lambda step: step.action, example.steps))\n",
    "\n",
    "    train_obs_v = torch.FloatTensor(train_obs)\n",
    "    train_act_v = torch.LongTensor(train_act)\n",
    "    return train_obs_v, train_act_v, reward_bound, reward_mean\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = DiscreteOneHotWrapper(gym.make(\"FrozenLake-v0\"))\n",
    "    env = gym.wrappers.Monitor(env, directory=\"mon\", force=True)\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "    objective = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(params=net.parameters(), lr=0.01)\n",
    "    writer = SummaryWriter(comment=\"-frozenlake-naive\")\n",
    "\n",
    "    for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
    "        obs_v, acts_v, reward_b, reward_m = filter_batch(batch, PERCENTILE)\n",
    "        optimizer.zero_grad()\n",
    "        action_scores_v = net(obs_v)\n",
    "        loss_v = objective(action_scores_v, acts_v)\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "        print(\"%d: loss=%.3f, reward_mean=%.1f, reward_bound=%.1f\" % (\n",
    "            iter_no, loss_v.item(), reward_m, reward_b))\n",
    "        writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
    "        writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n",
    "        writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n",
    "        if reward_m > 0.8:\n",
    "            print(\"Solved!\")\n",
    "            break\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In CartPole, every step of the environment gives us the\n",
    "reward 1.0, until the moment that the pole falls. So, the longer our agent\n",
    "balanced the pole, the more reward it obtained. Due to randomness in our agent's\n",
    "behavior, different episodes were of different lengths, which gave us a pretty\n",
    "normal distribution of the episodes' rewards. After choosing a reward boundary,\n",
    "we rejected less successful episodes and learned how to repeat better ones (by\n",
    "training on successful episodes' data).\n",
    "\n",
    "In the FrozenLake environment, episodes and their reward look different. We get\n",
    "the reward of 1.0 only when we reach the goal, and this reward says nothing\n",
    "about how good each episode was. Was it quick and efficient or did we make\n",
    "four rounds on the lake before we randomly stepped into the final cell? We don't\n",
    "know, it's just 1.0 reward and that's it. The distribution of rewards for our\n",
    "episodes are also problematic. There are only two kinds of episodes possible,\n",
    "with zero reward (failed) and one reward (successful), and failed episodes will\n",
    "obviously dominate in the beginning of the training. So, our percentile selection\n",
    "of \"elite\" episodes is totally wrong and gives us bad examples to train on. This is\n",
    "the reason for our training failure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### frozenlake tweaked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- **Larger batches of played episodes**: FrozenLake requires at least 100 just to get some successful episodes.\n",
    "- **Discount factor applied to reward**: To make the total reward for the episode depend on episode length, and add variety in episodes, In this case, the reward for shorter episodes will be higher than the reward for longer ones\n",
    "- **Keeping \"elite\" episodes for a longer time**:In FrozenLake, a successful episode is a much rarer animal, so we need to keep them for several iterations to train on them.\n",
    "- **Decrease learning rate**: This will give our network time to average more training samples.\n",
    "- **Much longer training time**: Due to the sparsity of successful episodes, and the random outcome of our actions, it's much harder for our network to get an idea of the best behavior to perform in any particular situation. To reach 50% successful episodes, about 5k training iterations are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T17:55:34.722829Z",
     "start_time": "2019-07-22T17:07:57.959847Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=1.371, reward_mean=0.010, reward_bound=0.000, batch=1\n",
      "100: loss=1.280, reward_mean=0.040, reward_bound=0.000, batch=181\n",
      "200: loss=1.165, reward_mean=0.070, reward_bound=0.254, batch=225\n",
      "300: loss=1.075, reward_mean=0.040, reward_bound=0.277, batch=229\n",
      "400: loss=1.071, reward_mean=0.060, reward_bound=0.000, batch=139\n",
      "500: loss=1.024, reward_mean=0.090, reward_bound=0.183, batch=227\n",
      "600: loss=1.011, reward_mean=0.060, reward_bound=0.045, batch=221\n",
      "700: loss=1.033, reward_mean=0.140, reward_bound=0.314, batch=220\n",
      "800: loss=1.021, reward_mean=0.030, reward_bound=0.229, batch=229\n",
      "900: loss=0.783, reward_mean=0.170, reward_bound=0.185, batch=217\n",
      "1000: loss=0.753, reward_mean=0.130, reward_bound=0.349, batch=226\n",
      "1100: loss=0.763, reward_mean=0.110, reward_bound=0.080, batch=219\n",
      "1200: loss=0.728, reward_mean=0.200, reward_bound=0.351, batch=228\n",
      "1300: loss=0.719, reward_mean=0.160, reward_bound=0.254, batch=226\n",
      "1400: loss=0.722, reward_mean=0.110, reward_bound=0.405, batch=230\n",
      "1500: loss=0.600, reward_mean=0.130, reward_bound=0.077, batch=214\n",
      "1600: loss=0.568, reward_mean=0.160, reward_bound=0.109, batch=220\n",
      "1700: loss=0.533, reward_mean=0.250, reward_bound=0.150, batch=212\n",
      "1800: loss=0.582, reward_mean=0.120, reward_bound=0.249, batch=229\n",
      "1900: loss=0.547, reward_mean=0.260, reward_bound=0.274, batch=224\n",
      "2000: loss=0.545, reward_mean=0.140, reward_bound=0.325, batch=229\n",
      "2100: loss=0.462, reward_mean=0.230, reward_bound=0.089, batch=215\n",
      "2200: loss=0.418, reward_mean=0.360, reward_bound=0.324, batch=225\n",
      "2300: loss=0.416, reward_mean=0.340, reward_bound=0.324, batch=225\n",
      "2400: loss=0.414, reward_mean=0.480, reward_bound=0.387, batch=184\n",
      "2500: loss=0.388, reward_mean=0.260, reward_bound=0.347, batch=231\n",
      "2600: loss=0.365, reward_mean=0.340, reward_bound=0.308, batch=227\n",
      "2700: loss=0.370, reward_mean=0.350, reward_bound=0.439, batch=231\n",
      "2800: loss=0.333, reward_mean=0.390, reward_bound=0.229, batch=216\n",
      "2900: loss=0.311, reward_mean=0.420, reward_bound=0.387, batch=225\n",
      "3000: loss=0.318, reward_mean=0.440, reward_bound=0.265, batch=223\n",
      "3100: loss=0.321, reward_mean=0.330, reward_bound=0.321, batch=227\n",
      "3200: loss=0.320, reward_mean=0.410, reward_bound=0.387, batch=228\n",
      "3300: loss=0.310, reward_mean=0.430, reward_bound=0.451, batch=231\n",
      "3400: loss=0.266, reward_mean=0.440, reward_bound=0.047, batch=180\n",
      "3500: loss=0.264, reward_mean=0.390, reward_bound=0.321, batch=227\n",
      "3600: loss=0.305, reward_mean=0.360, reward_bound=0.282, batch=207\n",
      "3700: loss=0.289, reward_mean=0.280, reward_bound=0.234, batch=228\n",
      "3800: loss=0.285, reward_mean=0.360, reward_bound=0.282, batch=216\n",
      "3900: loss=0.288, reward_mean=0.330, reward_bound=0.353, batch=229\n",
      "4000: loss=0.207, reward_mean=0.460, reward_bound=0.353, batch=229\n",
      "4100: loss=0.193, reward_mean=0.520, reward_bound=0.282, batch=219\n",
      "4200: loss=0.181, reward_mean=0.360, reward_bound=0.150, batch=204\n",
      "4300: loss=0.181, reward_mean=0.440, reward_bound=0.329, batch=227\n",
      "4400: loss=0.168, reward_mean=0.430, reward_bound=0.311, batch=226\n",
      "4600: loss=0.133, reward_mean=0.440, reward_bound=0.229, batch=216\n",
      "4700: loss=0.152, reward_mean=0.450, reward_bound=0.202, batch=215\n",
      "4800: loss=0.192, reward_mean=0.370, reward_bound=0.229, batch=218\n",
      "4900: loss=0.175, reward_mean=0.410, reward_bound=0.338, batch=224\n",
      "5000: loss=0.166, reward_mean=0.420, reward_bound=0.430, batch=223\n",
      "5100: loss=0.167, reward_mean=0.380, reward_bound=0.349, batch=213\n",
      "5200: loss=0.171, reward_mean=0.260, reward_bound=0.254, batch=216\n",
      "5300: loss=0.166, reward_mean=0.350, reward_bound=0.314, batch=182\n",
      "5400: loss=0.152, reward_mean=0.400, reward_bound=0.314, batch=224\n",
      "5500: loss=0.169, reward_mean=0.370, reward_bound=0.396, batch=227\n",
      "5600: loss=0.172, reward_mean=0.320, reward_bound=0.308, batch=222\n",
      "5700: loss=0.151, reward_mean=0.380, reward_bound=0.187, batch=215\n",
      "5800: loss=0.174, reward_mean=0.380, reward_bound=0.324, batch=218\n",
      "5900: loss=0.198, reward_mean=0.350, reward_bound=0.206, batch=221\n",
      "6000: loss=0.189, reward_mean=0.300, reward_bound=0.282, batch=224\n",
      "6100: loss=0.173, reward_mean=0.360, reward_bound=0.138, batch=213\n",
      "6200: loss=0.190, reward_mean=0.440, reward_bound=0.478, batch=227\n",
      "6300: loss=0.151, reward_mean=0.380, reward_bound=0.282, batch=217\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-66252c7ed308>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mfull_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0miter_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterate_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0mreward_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mfull_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_bound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_batch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPERCENTILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-66252c7ed308>\u001b[0m in \u001b[0;36miterate_batches\u001b[0;34m(env, net, batch_size)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mact_probs_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mact_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mact_probs_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mact_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mnext_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mepisode_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "import gym.spaces\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 100 #Larger batches of played episodes\n",
    "PERCENTILE = 30\n",
    "GAMMA = 0.9 #Discount factor applied to reward\n",
    "\n",
    "\n",
    "class DiscreteOneHotWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(DiscreteOneHotWrapper, self).__init__(env)\n",
    "        assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
    "        self.observation_space = gym.spaces.Box(0.0, 1.0, (env.observation_space.n, ), dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        res = np.copy(self.observation_space.low)\n",
    "        res[observation] = 1.0\n",
    "        return res\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])\n",
    "\n",
    "\n",
    "def iterate_batches(env, net, batch_size):\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    obs = env.reset()\n",
    "    sm = nn.Softmax(dim=1)\n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor([obs])\n",
    "        act_probs_v = sm(net(obs_v))\n",
    "        act_probs = act_probs_v.data.numpy()[0]\n",
    "        action = np.random.choice(len(act_probs), p=act_probs)\n",
    "        next_obs, reward, is_done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        episode_steps.append(EpisodeStep(observation=obs, action=action))\n",
    "        if is_done:\n",
    "            batch.append(Episode(reward=episode_reward, steps=episode_steps))\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            next_obs = env.reset()\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        obs = next_obs\n",
    "\n",
    "\n",
    "def filter_batch(batch, percentile):\n",
    "    disc_rewards = list(map(lambda s: s.reward * (GAMMA ** len(s.steps)), batch)) #discount factor\n",
    "    reward_bound = np.percentile(disc_rewards, percentile)\n",
    "\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    elite_batch = []\n",
    "    for example, discounted_reward in zip(batch, disc_rewards):\n",
    "        if discounted_reward > reward_bound:\n",
    "            train_obs.extend(map(lambda step: step.observation, example.steps))\n",
    "            train_act.extend(map(lambda step: step.action, example.steps))\n",
    "            elite_batch.append(example)\n",
    "\n",
    "    return elite_batch, train_obs, train_act, reward_bound # elite_batch\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    random.seed(12345)\n",
    "    env = DiscreteOneHotWrapper(gym.make(\"FrozenLake-v0\"))\n",
    "    # env = gym.wrappers.Monitor(env, directory=\"mon\", force=True)\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "    objective = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(params=net.parameters(), lr=0.001) #the learning rate decreased 10 times\n",
    "    writer = SummaryWriter(comment=\"-frozenlake-tweaked\")\n",
    "\n",
    "    full_batch = []\n",
    "    for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
    "        reward_mean = float(np.mean(list(map(lambda s: s.reward, batch))))\n",
    "        # use previous \"elite\" episodes \n",
    "        full_batch, obs, acts, reward_bound = filter_batch(full_batch + batch, PERCENTILE)\n",
    "        if not full_batch:\n",
    "            continue\n",
    "        obs_v = torch.FloatTensor(obs)\n",
    "        acts_v = torch.LongTensor(acts)\n",
    "        full_batch = full_batch[-500:]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        action_scores_v = net(obs_v)\n",
    "        loss_v = objective(action_scores_v, acts_v)\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "        if iter_no%100==0:\n",
    "            print(\"%d: loss=%.3f, reward_mean=%.3f, reward_bound=%.3f, batch=%d\" % (\n",
    "            iter_no, loss_v.item(), reward_mean, reward_bound, len(full_batch)))\n",
    "        writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
    "        writer.add_scalar(\"reward_mean\", reward_mean, iter_no)\n",
    "        writer.add_scalar(\"reward_bound\", reward_bound, iter_no)\n",
    "        if reward_mean > 0.8:\n",
    "            print(\"Solved!\")\n",
    "            break\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### frozenlake nonslippery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T18:02:40.994093Z",
     "start_time": "2019-07-22T18:02:15.126264Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drdh/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Trying to monitor an environment which has no 'spec' set. This usually means you did not create it via 'gym.make', and is recommended only for advanced users.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10: loss=1.358, reward_mean=0.040, reward_bound=0.000, batch=16\n",
      "20: loss=1.308, reward_mean=0.020, reward_bound=0.000, batch=34\n",
      "30: loss=1.265, reward_mean=0.070, reward_bound=0.000, batch=71\n",
      "40: loss=1.228, reward_mean=0.060, reward_bound=0.000, batch=140\n",
      "50: loss=1.171, reward_mean=0.120, reward_bound=0.122, batch=220\n",
      "60: loss=1.013, reward_mean=0.190, reward_bound=0.314, batch=205\n",
      "70: loss=0.849, reward_mean=0.240, reward_bound=0.387, batch=213\n",
      "80: loss=0.690, reward_mean=0.330, reward_bound=0.328, batch=181\n",
      "90: loss=0.696, reward_mean=0.440, reward_bound=0.000, batch=86\n",
      "100: loss=0.557, reward_mean=0.560, reward_bound=0.387, batch=135\n",
      "110: loss=0.404, reward_mean=0.600, reward_bound=0.430, batch=99\n",
      "Solved!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "import gym.spaces\n",
    "import gym.wrappers\n",
    "import gym.envs.toy_text.frozen_lake\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 100\n",
    "PERCENTILE = 30\n",
    "GAMMA = 0.9\n",
    "\n",
    "\n",
    "class DiscreteOneHotWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(DiscreteOneHotWrapper, self).__init__(env)\n",
    "        assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
    "        self.observation_space = gym.spaces.Box(0.0, 1.0, (env.observation_space.n, ), dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        res = np.copy(self.observation_space.low)\n",
    "        res[observation] = 1.0\n",
    "        return res\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])\n",
    "\n",
    "\n",
    "def iterate_batches(env, net, batch_size):\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    obs = env.reset()\n",
    "    sm = nn.Softmax(dim=1)\n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor([obs])\n",
    "        act_probs_v = sm(net(obs_v))\n",
    "        act_probs = act_probs_v.data.numpy()[0]\n",
    "        action = np.random.choice(len(act_probs), p=act_probs)\n",
    "        next_obs, reward, is_done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        episode_steps.append(EpisodeStep(observation=obs, action=action))\n",
    "        if is_done:\n",
    "            batch.append(Episode(reward=episode_reward, steps=episode_steps))\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            next_obs = env.reset()\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        obs = next_obs\n",
    "\n",
    "\n",
    "def filter_batch(batch, percentile):\n",
    "    disc_rewards = list(map(lambda s: s.reward * (GAMMA ** len(s.steps)), batch))\n",
    "    reward_bound = np.percentile(disc_rewards, percentile)\n",
    "\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    elite_batch = []\n",
    "    for example, discounted_reward in zip(batch, disc_rewards):\n",
    "        if discounted_reward > reward_bound:\n",
    "            train_obs.extend(map(lambda step: step.observation, example.steps))\n",
    "            train_act.extend(map(lambda step: step.action, example.steps))\n",
    "            elite_batch.append(example)\n",
    "\n",
    "    return elite_batch, train_obs, train_act, reward_bound\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    random.seed(12345)\n",
    "    env = gym.envs.toy_text.frozen_lake.FrozenLakeEnv(is_slippery=False)\n",
    "    #env = gym.wrappers.TimeLimit(env, max_episode_steps=100) # env.spec is NoneType,error\n",
    "    env = DiscreteOneHotWrapper(env)\n",
    "    env = gym.wrappers.Monitor(env, directory=\"mon\", force=True)\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "    objective = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(params=net.parameters(), lr=0.001)\n",
    "    writer = SummaryWriter(comment=\"-frozenlake-nonslippery\")\n",
    "\n",
    "    full_batch = []\n",
    "    for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
    "        reward_mean = float(np.mean(list(map(lambda s: s.reward, batch))))\n",
    "        full_batch, obs, acts, reward_bound = filter_batch(full_batch + batch, PERCENTILE)\n",
    "        if not full_batch:\n",
    "            continue\n",
    "        obs_v = torch.FloatTensor(obs)\n",
    "        acts_v = torch.LongTensor(acts)\n",
    "        full_batch = full_batch[-500:]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        action_scores_v = net(obs_v)\n",
    "        loss_v = objective(action_scores_v, acts_v)\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "        if iter_no%10==0:\n",
    "            print(\"%d: loss=%.3f, reward_mean=%.3f, reward_bound=%.3f, batch=%d\" % (\n",
    "            iter_no, loss_v.item(), reward_mean, reward_bound, len(full_batch)))\n",
    "        writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
    "        writer.add_scalar(\"reward_mean\", reward_mean, iter_no)\n",
    "        writer.add_scalar(\"reward_bound\", reward_bound, iter_no)\n",
    "        if reward_mean > 0.8:\n",
    "            print(\"Solved!\")\n",
    "            break\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Theoretical background of the cross-entropy method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The basis of the cross-entropy method lies in the importance sampling theorem, which states this:\n",
    "$$\n",
    "\\mathbb{E}_{x\\sim p(x)}[H(x)]=\\mathbb{E}_{x\\sim q(x)}\\left[\\frac{p(x)}{q(x)}H(x)  \\right]\n",
    "$$\n",
    "In our RL case, $H(x)$ is a reward value obtained by some policy $x$ and $p(x)$ is a distribution of all possible policies. We don't want to maximize our reward by searching all possible policies, instead we want to find a way to approximate $p(x)H(x)$ by $q(x)$, iteratively minimizing the distance between them. The distance between two probability distributions is calculated by KL-divergence which is as follows:\n",
    "\n",
    "$$\n",
    "KL(p_1(x)||p_2(x))=\\mathbb{E}_{x\\sim p_1(x)}\\log \\frac{p_1(x)}{p_2(x)} = \\mathbb{E}_{x\\sim p_1(x)}{[\\log p_1(x)]}-\\mathbb{E}_{x\\sim p_1(x)}{[\\log p_2(x)]}\n",
    "$$\n",
    "\n",
    "The first term in KL is called **entropy** and doesn't depend on that, so could be\n",
    "omitted during the minimization. The second term is called **cross-entropy** and is\n",
    "a very common optimization objective in DL.\n",
    "\n",
    "Combining both formulas, we can get an iterative algorithm, which starts with $q_0(x)=p(x)$ and on every step improves. This is an approximation of $p(x)H(x)$ with an update:\n",
    "\n",
    "$$\n",
    "q_{i+1}(x)=\\arg\\min_{q_{i+1}(x)}-\\mathbb{E}_{x\\sim q_i (x)}\\frac{p(x)}{q_i(x)}H(x)\\log q_{i+1}(x)\n",
    "$$\n",
    "\n",
    "This is a generic cross-entropy method, which can be significantly simplified in\n",
    "our RL case. Firstly, we replace our $H(x)$ with an indicator function, which is 1\n",
    "when the reward for the episode is above the threshold and 0 if the reward is\n",
    "below. Our policy update will look like this:\n",
    "\n",
    "$$\n",
    "\\pi_{i+1}(a|s)=\\arg\\min_{\\pi_{i+1}(a|s)}-\\mathbb{E}_{z\\sim \\pi_{i+1}(a|s)}[R(z)\\ge \\psi_i]\\log \\pi_{i+1}(a|s)\n",
    "$$\n",
    "\n",
    "Strictly speaking, the preceding formula misses the normalization term, but it\n",
    "still works in practice without it. So, the method is quite clear: we sample\n",
    "episodes using our current policy (starting with some random initial policy) and\n",
    "minimize the negative log likelihood of the most successful samples and our\n",
    "policy.\n",
    "\n",
    "There is a whole book dedicated to this method, written by Dirk P. Kroese. A\n",
    "shorter description can be found in the Cross-Entropy Method paper by Dirk\n",
    "P.Kroese (https://people.smp.uq.edu.au/DirkKroese/ps/eormsCE.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T15:38:17.504633Z",
     "start_time": "2019-07-21T15:38:17.501013Z"
    }
   },
   "source": [
    "## Tabular Learning and the Bellman Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stocks Trading Using RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradients -- An Alternative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Actor-Critic Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T15:40:36.997768Z",
     "start_time": "2019-07-21T15:40:36.994182Z"
    }
   },
   "source": [
    "## Asynchronous Advantage Actor-Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbots Training with RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Action Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T15:42:20.701409Z",
     "start_time": "2019-07-21T15:42:20.697313Z"
    }
   },
   "source": [
    "## Trust Regions -- TRPO,PPO and ACKTR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T15:42:55.115122Z",
     "start_time": "2019-07-21T15:42:55.111679Z"
    }
   },
   "source": [
    "## Black-Box Optimization in RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond Model-Free -- Imagination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlphaGo Zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "300px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
